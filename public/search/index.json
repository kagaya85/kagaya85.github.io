[{"content":"随着go 1.18版本的发布，go社区终于迎来了期盼已久的正式泛型语法，然而我在浏览关于1.18的changelog时发现1.18对于slice的扩容策略也做了一些修改，刚好我最近正在看draven大佬的新书也讲到了slice底层的源码的一些运行逻辑，在好奇心的驱使下，想知道社区究竟改动了什么以及为什么作出这些改动，于是翻开1.18的源码，便有了这篇文章。\nGrowslice In Go 1.17 首先我们先简单回顾一下go 1.17及以前的切片扩容策略，这部分的主要逻辑在src/runtime/slice.go中的growslice函数（省略部分代码，下同）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func growslice(et *_type, old slice, cap int) slice { ... newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { if old.cap \u0026lt; 1024 { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow // and prevent an infinite loop. for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += newcap / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap \u0026lt;= 0 { newcap = cap } } } ... } 上面代码的扩容策略可以简述为以下三个规则：\n当期望容量 \u0026gt; 两倍的旧容量时，直接使用期望容量作为新切片的容量 如果旧容量 \u0026lt; 1024（注意这里单位是元素个数）,那么直接翻倍旧容量 如果旧容量 \u0026gt; 1024，那么会进入一个循环，每次增加25%直到大于期望容量 可以看到，原来的go对于切片扩容后的容量判断有一个明显的magic number：1024，在1024之前，增长的系数是2，而1024之后则变为1.25。关于为什么会这么设计，社区的相关讨论1给出了几点理由：\n如果只选择翻倍的扩容策略，那么对于较大的切片来说，现有的方法可以更好的节省内存。 如果只选择每次系数为1.25的扩容策略，那么对于较小的切片来说扩容会很低效。 之所以选择一个小于2的系数，在扩容时被释放的内存块会在下一次扩容时更容易被重新利用。 关于3的原理也很简单，对于一个2为公比的等比数列，那么其前（n-1）项和必然小于第n项： $$ \\sum(2^0,2^1,2^2,\u0026hellip;,2^{(n-1)}) = 2^{n-1}-1 \u0026lt; 2^n $$ 也就是说，如果按2为系数进行扩容，那么每一次扩容所需要的空间都大于之前释放的所有空间之和，那么也就谈不上重新利用了。\n可以看到当前方法也是作出了一些权衡，希望同时兼顾扩容效率和内存利用率，而以1024为分界点多半也是写代码人的个人喜好。\nWhat\u0026rsquo;s the problem？ 上面的扩容策略一直使用了许多年，但它仍然存在一个问题：那就是扩容策略并不是一个单调函数。\n对于扩容策略不是单调函数，我们用下面的代码来做验证：\n1 2 3 4 5 func main() { for i := 0; i \u0026lt; 2000; i += 100 { fmt.Println(i, cap(append(make([]bool, i), true))) } } 这段代码通过申请一个长度为i的切片，然后对其append一个元素来触发扩容。每次实验新增加100个元素，同时打印扩容前后的大小（单位byte）：\n其中蓝色的线代表扩容前的每次容量大小变化，橙色线代表对应扩容后的大小变化，横坐标为实验序号，纵坐标为容量大小。\n可以看到在第11次与第12次扩容后，扩容后的容量反而出现了下降，由于存在内存对齐，所以最后的容量会向上取一个合理的数值。\n扩容前容量 扩容后容量 第11次 1000 2048 第12次 1100 1408 Growslice In Go 1.18 接下来我们回到刚刚发布的go 1.18版本中，在1.18中，优化了切片扩容的策略2，让底层数组大小的增长更加平滑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func growslice(et *_type, old slice, cap int) slice { ... newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { const threshold = 256 if old.cap \u0026lt; threshold { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow // and prevent an infinite loop. for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { // Transition from growing 2x for small slices // to growing 1.25x for large slices. This formula // gives a smooth-ish transition between the two. newcap += (newcap + 3*threshold) / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap \u0026lt;= 0 { newcap = cap } } } ... } 重点关注第6行以后的代码，修改原来的1024为一个值为256的threshold，大于阈值后的新容量的计算也由原来的 $$ newcap = oldcap + \\frac{oldcap}{4} \\space\\space\\space\\space\\space \\text{if $oldcap \\geq 1024$} $$ 变为了 $$ newcap = oldcap + \\frac{oldcap + 3 \\times threshold}{4} \\space\\space\\space\\space\\space \\text{if $oladcap \\geq threshold$} $$ 通过减小阈值并固定增加一个常数，使得优化后的扩容的系数在阈值前后不再会出现从2到1.25的突变，该commit作者给出了几种原始容量下对应的“扩容系数”：\n原始容量 扩容系数 256 2.0 512 1.63 1024 1.44 2048 1.35 4096 1.30 我们重新验证一下前一节中的代码，可以看到1.18中优化后的扩容策略可以保证是一个单调函数\nThe End 这次问题源自于社区成员对于扩容机制中魔数的疑问，而对于整个runtime来说，slice的扩容只是其中最简单的冰山一角，即使如此，我们也看到社区对于细节问题的重视，通过一点一点的优化让golang能够不断进步。对于gopher来说，我们在使用golang的时候往往不会注意其背后的运行原理，因为go已经把一切都做好了，这也是我喜欢golang的原因，在“少即是多”的原则之下，go把许多复杂的运行机制很好的隐藏在runtime的源码之中，从而带给gopher最好的编程体验。而研究runtime的运行机制，便能够发现许多类似这种问题，通过研究社区的解决方法，也不失为一种乐趣。\nReferences 封面来自: https://www.pixiv.net/artworks/96944986\nslices grow at 25% after 1024 but why 1024? (google.com)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nruntime: make slice growth formula a bit smoother · golang/go@2dda92f (github.com)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-03-27T14:04:09+08:00","image":"https://blog.kagaya.fun/p/2022/go118-new-growslice/96944986_p0_hu_b8d6fc274e6ff578.jpeg","permalink":"https://blog.kagaya.fun/p/2022/go118-new-growslice/","title":"Go1.18：新的切片扩容策略"},{"content":" 论文标题：MicroRank: End-to-End Latency Issue Localization with Extended Spectrum Analysis in Microservice Environments\n论文来源：WWW21\n论文链接：MicroRank: End-to-End Latency Issue Localization with Extended Spectrum Analysis in Microservice Environments | Proceedings of the Web Conference 2021 (acm.org)\n源码链接：\nTL;DR 这篇文章提出了一个利用谱分析实现根因定位的方法：MicroRank，通过收集分析异常合正常的trace数据，来定位延迟问题的根因，当MicroRank的异常检测器检测到发生延迟问题时，就会触发根因定位的过程：MicroRank首先会利用一个PageRank的打分器不同trace进行重要性评估，然后谱分析（spectrum analysis）利用打分器得出的权重来计算根因排序。通过在广泛使用的开源微服务系统和生产系统中，对比其他根因定位方法，如CauseInfer和Sieve，MicroRank可以得到更加有效的结果。\nAlgorithm Spectrum-based fault localization Spectrum-based fault localization (SBFL) ，谱分析在故障定位中是比较有效且简单的方法。简单来说，在一个程序$P$和其对应的测试集合中，对于程序中的每一个元素$O\\in P$，定义一个四元组：\n$$ (O_{ef},O_{ep},O_{nf},O_{np}) $$ $O_{ef}$: 覆盖O的测试case中失败的数量\n$O_{ep}$: 覆盖O的测试case中通过的数量\n$O_{nf}$: 没有覆盖O的测试case中失败的数量\n$O_{np}$: 没有覆盖O的测试case中通过的数量\n类似软件测试中的思路，当经过一个服务执行的异常trace越多且执行正常trace的数量越少，那么这个服务越有可能是导致异常的根因。\nSystem MicroRank主要分为四个部分：Anomaly Detector, Data Preparator, PageRank Scorer 和 Weighted Spectrum Ranker\n系统通过一个时间窗口（文中是30s），如果实时的异常大于一个期望值，就认为出现了异常，并触发根因定位，由Data Preparator来区分时间窗口内的正常/异常trace，并且构建调用图，由一个基于PageRank的打分器为每一个operation（即服务接口，下文同）进行异常分数和正常分数的打分，输入到Spectrum Ranker不断的更新权重进行排序\nAnomaly Detector 异常检测器主要计算每个operation在一段时间（比如一个小时）内的平均处理时间 $\\mu_o$ 和标准差 $\\delta_o$ ,文中特别说明了只有在出现预测偏差的时候才需要更新上述的两个值。\n在一个时间窗口内，异常检测需要统计trace涉及的operation以及对应的数量 $count_o$ ，通过下面式子计算期望一条trace的期望响应时间 $L_{excepted}$ ：\n$$ L_{excepted} = \\sum count_o \\times (\\mu_o + n \\times \\delta_o) $$ 一条trace的期望响应时间计算为各个operation的n sigma的乘积的和，在文中$n=1.5$，如果实际trace的响应时间大于期望值，则会被认为是异常trace，同时触发根因定位并刷新时间窗口。\n这里其实存在一些问题：对于OpenTracing格式的span来说，各个span的duration是包含了子span的duration，理论上只需要计算root span的duration即可，不需要累加每个span的duration。\nPageRank Scorer 文中为了提高谱分析的性能，增加了一个基于PageRank的打分器，分别对异常和正常的trace进行游走计算权重信息。基本思想就是对trace上的每一个operation进行打分来评价trace的重要性，越能够帮助找到根因的trace重要性会越高：\n如果一个operation被越多的异常trace经过，那么它很有可能是根因 如果一个异常trace经过了越少的operation，那么这条trace的重要性越高，因为它包含根因的范围更小（更容易定位根因） 如果一种trace出现的数量越少，那么也需要给予更高的重要性 经过PageRank会得出一个得分向量$v$： $$ v^{(q)} = dAv^{(q-1)} + (1-d) \\cdot u $$ $d$是参数，$q$是迭代次数，每次进行迭代计算都会更加接近最终的结果，除此之外这个向量还需要矩阵$A$与向量$u$，其中$A$是关于operation和trace的矩阵：\n$$ A = \\left[ \\begin{array}{c|c} \\overbrace{A_{oo}}^{operations} \u0026amp; \\overbrace{A_{ot}}^{traces} \\ \\hline A_{to} \u0026amp; 0 \\end{array} \\right] $$\n其中$A_{st}$代表了从s出发到t结束的矩阵，矩阵中的值代表转移的概率，即$\\frac{1}{相邻节点的数量}$。作者把每一次请求看作图中的一个单独的节点，构造了operation-trace的关系图，如下面图3到图6的变化：\n基于图6，我们便能得出一个关于向量$[front, recommend, checkout, product, req1, req2, req4]$的异常关系矩阵：\n$$ A = \\left[ \\begin{array}{cccc|ccc} 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;1/2\u0026amp;1/3\u0026amp;1/3 \\ 1/3\u0026amp;0\u0026amp;1/3\u0026amp;0\u0026amp;0\u0026amp;1/3\u0026amp;0 \\ 1/3\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;1/3 \\ 1/3\u0026amp;1\u0026amp;1\u0026amp;0\u0026amp;1/2\u0026amp;1/3\u0026amp;1/3 \\ \\hline 1/3\u0026amp;0\u0026amp;0\u0026amp;1/3\u0026amp;0\u0026amp;0\u0026amp;0 \\ 1/3\u0026amp;0\u0026amp;1\u0026amp;1/3\u0026amp;0\u0026amp;0\u0026amp;0 \\ 1/3\u0026amp;1\u0026amp;0\u0026amp;1/3\u0026amp;0\u0026amp;0\u0026amp;0 \\ \\end{array} \\right] $$\n作者认为系统的接口之间的\u0011\u0011调用关系图，即$A_{oo}$， 是相对稳定的，而根因往往对operation-trace图，即$A_{ot}$和$A_{to}$影响更大，于是通过增加一个参数${\\omega}(0\\le\\omega\\le1)$来降低调用关系图的影响：\n$$ A = \\left[ \\begin{array}{c|c} \\omega A_{oo} \u0026amp; A_{ot} \\ \\hline A_{to} \u0026amp; 0 \\end{array} \\right] $$\n文中用向量$u$来计算trace覆盖范围和种类数量的影响：$u = [u_o^T, u_t^T]^T$，其中只需要考虑trace的影响，故operation的向量$u_o$是$\\vec{0}$。对$u_t$来说，每一条异常trace的重要性得分$\\theta_i$遵循以下计算方法： $$ \\theta_i = \\varphi\\cdot\\frac{n_i^{-1}}{\\sum n_j^{-1}} + (1-\\varphi)\\cdot\\frac{k_i^{-1}}{\\sum k_j^{-1}} $$ $n_i$为异常trace $i$经过operation的数量（覆盖范围），$k_i$则为该异常trace的数量。$0\\le\\varphi\\le1$，文中默认为0.5，用于平衡trace范围和trace数量的参数。对于正常的trace，则经过operation的数量并不能够反应重要性（因为它们的结构都是一样的），因此只需要计算$\\frac{k_i^{-1}}{\\sum k_j^{-1}}$即可。\n当计算出矩阵$A$与向量$u$后，则可以计算pagerank分数，初始化$v$: $$ v^{(0)}=[v_o^T, v_t^T]^T=[\\frac{1}{N_o},\\frac{1}{N_o},\\cdots,\\frac{1}{N_o},\\frac{1}{N_t},\\frac{1}{N_t},\\cdots,\\frac{1}{N_t}]^T $$ $N_o$和$N_t$分别是operation数量和trace种类的数量（上文例子中的req1,2,3则为三种trace），通过（3）式迭代计算（实例代码中迭代次数为25），最后得出正常的评分P和异常trace评分向量\nWeighted Spectrum Ranker 最后，按照谱分析的方法，计算开头式(1)中提到的四个值：\n$$ \\begin{array} \\ O_{ef} = F * N_{ef}, \u0026amp; O_{nf} = F * (N_f - N_{ef}) \\ O_{ep} = P * N_{ep}, \u0026amp; O_{np} = P * (N_p - N_{ep}) \\end{array} $$\n之后可以根据下表所示的方法计算对应的值，作者没有解释如何选择，而是通过实验对比各种计算方法的效果\nExperiments 作者通过实验模拟了在线故障定位的流程：\n不同计算方法的对比：\nThoughts MicroRank方法只适用于Latency类型的异常，对于请求失败等结构异常 作者认为调用关系是稳定的，然而现实情况是可能业务系统中只有部分系统能够维持稳定的调用关系 文中的节点operation其实既可以是服务也可以是服务实例或者接口，作者在这一块的区别并没有详细阐述 另外在计算trace latency时也存在一些问题，对于OpenTracing规范的trace数据而言，最顶级的root span已经包含了整条trace的耗时数据，在文中却用了所有span累加的方法计算总耗时。 文中把调用trace当作节点与operation统一形成一张依赖图的方式比较有借鉴意义，谱分析的方式虽然简单淳朴，当时通过复现实验表明在大多数场景下还是最有效的。 作者虽然讨论了trace数量对结果的影响，但是没有讨论对性能的影响，对于在线方法来说，性能往往也是需要考虑的一点，在我复现的实验中，利用工业界的实际trace，5mins的数量级大概在万级别，此时在pagerank的性能就会非常差，往往需要几十个小时，所以还有不少改进的空间 ","date":"2022-02-08T15:04:55+08:00","image":"https://blog.kagaya.fun/p/2022/microrank/ScreenShot_2022-02-12_at_23.49.58@2x_hu_49693aa28ce808b5.png","permalink":"https://blog.kagaya.fun/p/2022/microrank/","title":"WWW21 | MicroRank: 微服务系统中谱分析方法实现的根因定位"},{"content":"写在前面 为什么我会突然想搞一台NAS呢，我这么问自己，仔细回想一下大概是因为自从番剧需要进行审核才能上映的政策出台后，我就准备了一块海康威视的1T ssd接一块败家之眼的m2硬盘盒用来存自己下载的番剧，同时也方便和朋友分享；后来阿b推出了放映室的功能，在网页上就可以很方便的和朋友一同看番看电影。但这样终究是不方便，首先放映室的内容只能先定于阿b已经拥有版权的内容，其次，你的朋友最好还要有大会员。好在我本身就是个爱折腾的人，刚好前段时间给群友搞了一个ts服务器来让群友们吹水聊天以及游戏语音，我就开始想能不能通过自建的服务器也实现一个类似的流媒体平台，随时和朋友分享自己私藏的电影番剧。\n假如有这样一个场景：某天有朋友（小姐姐）想看个电影但苦于没有资源，又不想去各种牛皮癣广告横飞的小网站，只得求助于你。你虽然早已掌握科学上网快速通过各种途径网罗了种子资源，熟练地运用bt下载器下载到了最新的4K HDR 10bit资源，那么你该怎么和远在网线另一端的她分享呢？当然你可以把bt文件丢过去耐心地教她怎么用FDM来下载，但是，如果此时我们能有一个自己的流媒体服务器，不就可以随时随地的给任何人分享硬盘里的片了吗？\n之前发现小区是有公网ip的，在暴力破解了宽带账号把光猫改成桥接以后，就可以直通外网了，尽管上行带宽并不富裕，但这么好的资源不拿来搭服务器简直浪费，而又“恰好”实验室发电脑是一台最新nuc11，配了11代的酷睿i7-1165G7，平时都用mac的我正愁该如何在毕业前发挥它的剩余价值，这波天时地利就被赶上了，于是就有了下面的折腾。\n方案探索 基于nas的媒体中心其实已经有不少成熟解决方案，Emby，Plex以及开源的Jellyfin都是最常用的。我们只需要吧nas里存的片共享给流媒体服务器，然后在外网暴露端口就行。这里主要又一些其他考虑：当前的主机需要在毕业后归还的，因此如何能够在保留大部分数据配置的情况下做到快速迁移整个nas系统便是我首先考虑的问题。同时之后肯定会再用一台自己组装的nas代替现在的系统，如何最大化复用硬件也是需要考虑的。因此在选择硬件和系统的时候就会有意留意以上两点。\n硬件 硬件选择比较简单，主要思想通过主机+硬盘盒raid的方式来实现：\n主机用的是白嫖的Intel NUC11PAHi7 猎豹峡谷，40w的TDP，硬盘从海鲜市场捡了两块4T的希捷酷狼，硬盘盒同样也是从海鲜市场捡的二手世特力双盘位带raid的硬盘盒，可以两块组硬raid1。同时配了APC的BK650M2 UPS来保证电源的稳定。\n刚拿到世特力的硬盘盒时发现卖家并没有给原装的脚垫，直接放到桌子上会有挺强烈的共振，怕搞坏硬盘临时垫了块纸板，放假回来后买了带一点厚度的减震贴贴到四个角，瞬间稳稳当当，噪音也减弱了不少。4T的酷狼是5400转，本身声音也很小，房间不大我都是直接放到床边，几乎听不到什么声音，室友也表示很安静没有什么影响。\n硬盘盒之后组装新nas后还可以拿来扩展新nas的空间，缺点就是usb不支持硬盘smart信息的查询，尽管它还带有一个esata的接口，不知道是否支持。\nnas底层系统选择 在选择系统之前首先还是要明确需求，对我来说，需求主要有以下几点：\n可以7x24稳定运行 可以硬件直通方便媒体服务器使用硬件编解码 可以开虚拟机 对Docker容器能有较好的支持 可以方便备份迁移 现在市面上几乎所有系统都可以用来做nas，因为对linux较为熟悉且为了获得原生的docker性能和体验，我在一开始就排除的windows系统，另外FreeNas（TrueNas）基于内存的缓存策略推荐使用大内存+ECC，硬件不具备也暂时不考虑，所以主要的选择在以下几个系统之间：\nEXSI、PVE等\n这些虚拟机管理平台对于多虚拟机的需求还是很容易满足的，其中exsi作为企业级的平台虽然对于个人使用时免费的，但对于家用硬件设备可能存在兼容性问题，nuc上装的exsi7.0就遇到了没有网卡驱动以及显卡无法直通的问题等，有些可以自己手动解决，也有一些问题需要官方支持目前来说是无解的，这也是耗费了我不少时间去验证最后得出的结论。总之如果不是较新的硬件，没有兼容性问题的前提下，exsi易用的交互界面以及一些成熟的功能（硬件直通，虚拟机快照备份）可能会有很大优势，除此之外基于linux的pve应该可以提高较好的软硬件兼容性\nOpenMediaVault\nomv是我在exsi上用来体验的nas系统，开源免费，相对于TrueNas它更接近于传统nas系统，磁盘权限管理、软raid等功能都是有的，docker管理需要借助portainer插件也可以满足。这方面我没有深入体验也不过多评论了。\nUbuntu对我来说就是上手容易，相关社区生态比较活跃。我一开始也是从ubuntu开始配置起，这篇博客也主要是讲ubuntu上的一些nas的基本配置方便之后查询。在使用了一段时间后，确实可以满足我的基本需求，docker来跑一些nas服务，kvm来配置虚拟机，同时还可以当远程linux主机跑一些程序或者配置远程开发环境。总之，既然是linux，那么它几乎可以满足一切想象的需求。但缺点也很明显，不好迁移，本来这样一个系统如果是建立在exsi上可以方便的做到迁移，但是由于前述的一些原因，以及对于多硬盘的管理也不太直观便捷，有一定的维护成本。在考虑的一段时间后不得不放弃这个方案，转而寻求一些更加稳定方便的方案，于是我便注意到了Unraid。\nUnraid\nUnraid最吸引我的只有两点：\n系统安装在一块u盘之中，不占用nuc里的固态硬盘，这样固态可以拿出来当缓存，迁移系统只需要把u盘硬盘装在另一台电脑，系统配置可以完整保留。 方便直观的磁盘阵列管理，特别是unraid的理念可以方便添加减少硬盘而不需要重新构建阵列，在不追求读写性能的前提下可以说十分好用了。 当然缺点就是它是一个收费的系统，价格也不便宜，我目前也在试用体验期，等一段时间的使用后可以考虑在聊聊感受。\n硬件直通 这里的需求主要还是用于硬件解码或者做软路由直通网卡用，前面也已经提到一些，如果不追求最新的硬件，那么exsi系统应该可以很好的满足需求，否则需要考虑原生的linux或者pve系统。另外硬件解码可以直接在docker容器里调用显卡，只需要关注所选用的系统是否有对应显卡的驱动即可。\n安装过程 这里简单记录一下当时主要的安装配置过程，想看最终效果的可以直接点这里\n配置UPS ups几乎是nas必备了，我选用的是APC的BK650M2，具体配置方法可以参考这篇文章：使用NUT解决BK650M2-CH失联问题（一） - 简书 (jianshu.com)\n1 sudo apt install nut 配置邮件提醒 1 sudo apt install mailutils 磁盘分区 fdisk，parted和gdisk fdisk:\n1 2 3 4 5 6 7 8 9 10 ➜ ~ sudo fdisk /dev/sda Welcome to fdisk (util-linux 2.34). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table. The size of this disk is 3.7 TiB (4000787030016 bytes). DOS partition table format cannot be used on drives for volumes larger than 2199023255040 bytes for 512-byte sectors. Use GUID partition table format (GPT). Created a new DOS disklabel with disk identifier 0x277fe383. 由于MBR分区表并不支持2T以上的容量，必须该用GPT分区表\n我们先使用parted分区:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ➜ ~ sudo parted /dev/sda GNU Parted 3.3 Using /dev/sda Welcome to GNU Parted! Type \u0026#39;help\u0026#39; to view a list of commands. (parted) print Error: /dev/sda: unrecognised disk label Model: ST4000VN 008-2DR166 (scsi) Disk /dev/sda: 4001GB Sector size (logical/physical): 512B/512B Partition Table: unknown Disk Flags: (parted) mklabel gpt (parted) print Model: ST4000VN 008-2DR166 (scsi) Disk /dev/sda: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags (parted) mkpart primary 0GB 4001GB (parted) print Model: ST4000VN 008-2DR166 (scsi) Disk /dev/sda: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 33.6MB 4001GB 4001GB primary (parted) quit Information: You may need to update /etc/fstab. 然后使用mkfs创建文件系统：\n1 2 3 4 5 6 7 8 9 10 ➜ ~ sudo mkfs.ext4 /dev/sda1 mke2fs 1.45.5 (07-Jan-2020) /dev/sda1 alignment is offset by 512 bytes. This may result in very poor performance, (re)-partitioning suggested. Creating filesystem with 976741831 4k blocks and 244187136 inodes Filesystem UUID: be74b765-92e7-4d99-a203-36863909129b Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848, 512000000, 550731776, 644972544 parted的默认起始扇区为65535，并不是8的整数倍，并且存在不少浪费，转用gdisk试一下\n1 2 3 4 5 6 7 8 9 10 11 12 13 Command (? for help): p Disk /dev/sda: 7814037168 sectors, 3.6 TiB Model: 008-2DR166 Sector size (logical/physical): 512/512 bytes Disk identifier (GUID): C32F7F72-31A6-4EAC-BB7A-5E9EF5E88865 Partition table holds up to 128 entries Main partition table begins at sector 2 and ends at sector 33 First usable sector is 34, last usable sector is 7814037134 Partitions will be aligned on 2048-sector boundaries Total free space is 2014 sectors (1007.0 KiB) Number Start (sector) End (sector) Size Code Name 1 2048 7814037134 3.6 TiB 8300 Linux filesystem lsblk看一下确认分区大小\n1 2 sda 8:0 0 3.7T 0 disk └─sda1 8:1 0 3.7T 0 part 通过mkfs在对应的分区上创建文件系统:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ➜ ~ sudo mkfs.ext4 /dev/sda1 mke2fs 1.45.5 (07-Jan-2020) Creating filesystem with 976754385 4k blocks and 244195328 inodes Filesystem UUID: 678f5019-216a-4d94-95ab-ca4b219b0a95 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848, 512000000, 550731776, 644972544 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done 自动挂载 自动挂载主要是通过查询对应磁盘的UUID，然后讲挂载命令写入系统的自动挂载文件\n1 2 3 4 blkid vim echo \u0026#34;UUID=e943fbb7-020a-4c64-a48a-2597eb2496df /data ext4 defaults 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab # 或者直接 vim /etc/fstab 编辑文件按上面👆的格式添加一行：设备UUID 挂载点 文件系统 挂载选项 是否备份 是否检测 sudo mount -a # 挂载所有fstab中的设备 配置nuc11显卡驱动 Intel对ubuntu 20.04的支持还是比较好的，可以按官方文档来装驱动，see：GPGPU: Ubuntu 20.04 (focal) (intel.com)\n1 2 3 4 sudo apt-get install \\ intel-opencl-icd \\ intel-level-zero-gpu level-zero \\ intel-media-va-driver-non-free libmfx1 安装驱动后没有看到/dev/dri文件夹，发现iris xe显卡有一些额外的包需要安装：\nsee: Ubuntu 20.04 no driver loaded for Intel Iris Xe Graphics - Ask Ubuntu\n1 2 3 sudo apt update sudo apt install linux-oem-20.04 sudo reboot WOL配置 前提是需要在主板bios里打开WOL设置，具体参考对应主板的bios设置方法\n1 2 3 4 5 6 # 安装工具 sudo apt install ethtool # 查看网卡设备，找到对应物理网卡 ip addr # 用 ethtool 查看是否支持Wake-On-Lan sudo ethtool eht0 可能会看到以下输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ethtool eth0 Settings for eth0: Supported ports: [ ] Supported link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full 1000baseT/Full 2500baseT/Full Supported pause frame use: Symmetric Supports auto-negotiation: Yes Supported FEC modes: Not reported Advertised link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full 1000baseT/Full 2500baseT/Full Advertised pause frame use: Symmetric Advertised auto-negotiation: Yes Advertised FEC modes: Not reported Speed: 2500Mb/s Duplex: Full Auto-negotiation: on Port: Twisted Pair PHYAD: 0 Transceiver: internal MDI-X: off (auto) Supports Wake-on: pumbg Wake-on: g Current message level: 0x00000007 (7) drv probe link Link detected: yes 主要关注这两行:\n1 2 Supports Wake-on: pumbg Wake-on: g 上面的pumbg代表支持唤醒的类型：\n下面表示当前的状态，如果是d代表disable，可以通过下面的方式开启：\n1 sudo ethtool -s eth0 wol g 其他配置可以参考这篇文章：Enabling Wake-On-LAN (In Ubuntu 20.10) | The Cloistered Monkey (necromuralist.github.io)\nexfat in ubuntu 由于我之前为了在多平台之间通用，移动硬盘被格式化为零exfat格式，linux默认读写exfat的时候偶尔会有问题，安装额外的包来获得更稳定的兼容：\n1 sudo apt install exfat-utils snap docker 配置好docker compose后，启动遇到了下面的问题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ➜ docker-compose up -d Starting jellyfin ... Starting webdav ... error Starting qbittorrent ... Starting qbittorrent ... error Starting jellyfin ... error ERROR: for qbittorrent Cannot start service qbittorrent: error while creating mount source path \u0026#39;/data/config/qbittorrent\u0026#39;: mkdir /data: read-only file system ERROR: for jellyfin Cannot start service jellyfin: error while creating mount source path \u0026#39;/data/config/jellyfin\u0026#39;: mkdir /data: read-only file system ERROR: for webdav Cannot start service webdav: error while creating mount source path \u0026#39;/data/share\u0026#39;: mkdir /data: read-only file system ERROR: for qbittorrent Cannot start service qbittorrent: error while creating mount source path \u0026#39;/data/config/qbittorrent\u0026#39;: mkdir /data: read-only file system ERROR: for jellyfin Cannot start service jellyfin: error while creating mount source path \u0026#39;/data/config/jellyfin\u0026#39;: mkdir /data: read-only file system ERROR: Encountered errors while bringing up the project. 查了一下，是snap自带docker的问题，删除ubuntu自带的docker重新安装官方的docker即可\nubuntu - Docker - mkdir read-only file system - Stack Overflow\nJellyfin 硬解 实验室给的这个nuc11的这块1165g7自带来一个蓝厂最新的iris Xe核显，96EU，性能应该是同代核显里最强的了，据说能有mx350的水平？不拿来当解码器真是浪费了。\n但问题也就出在这块核显实在太新了，需要新的驱动程序，配置方式也有一些不同，因此考虑显卡直通之前，一定先要调研好软硬件的兼容性。\nJellyfin的官方文档：Hardware Acceleration | Documentation - Jellyfin Project，中有这么一段话：\n1 For Intel Comet Lake or newer iGPUs, the legacy i965 VA-API driver is incompatible with your hardware. Please follow the instructions from Configuring Intel QuickSync(QSV) on Debian/Ubuntu to get the newer iHD driver. 所以不能在使用之前的VAAPI，转而使用最新的QuickSync方式：\n11代cpu核显需要使用新的驱动，而且因为licenses的原因docker镜像里不能包含驱动，所以需要手动安装：\n1 2 sudo apt update sudo apt install vainfo intel-media-va-driver-non-free -y 安装好驱动后，ls /dev/dri检查一下是否有对应的目录\n1 2 ➜ ~ ls /dev/dri by-path card0 renderD128 然后就可以在docker-compose文件中把显卡驱动共享给容器，同时记得需要给容器添加到render的用户组里\n1 2 3 4 5 6 7 8 9 10 11 12 13 version: \u0026#34;3.9\u0026#34; services: jellyfin: image: linuxserver/jellyfin:10.7.7 container_name: jellyfin ports: - ... group_add: - 109 # add to render group, see: cat /etc/group | grep render environment: - ... devices: - /dev/dri:/dev/dri # Intel 集显驱动 或者把当前用户添加到render组同时通过设置容器内的启动用户为当前用户来实现统一的目的\n1 sudo usermod -a -G render [username] 解码 2160p HEVC 10bit视频测试一下，安装intel-gpu-tool查看占用，可以看到确实有用到显卡的解码性能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 sudo apt install intel-gpu-tools sudo intel-gpu-top intel-gpu-top - 1100/1299 MHz; 0% RC6; ----- (null); 3610 irqs/s IMC reads: ------ (null)/s IMC writes: ------ (null)/s ENGINE BUSY MI_SEMA MI_WAIT Render/3D/0 100.00% |████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 36% 0% Blitter/0 0.00% | | 0% 0% Video/0 46.35% |████████████████████████████████████████████████████▎ | 0% 0% Video/1 39.64% |████████████████████████████████████████████▊ | 0% 0% VideoEnhance/0 38.52% |███████████████████████████████████████████▌ | 0% 0% qBittorrent 直接访问页面会提示Unauthorized：qBittorent UI returns 401 (blank page) when accessed through Kubernetes proxy · Issue #8095 · qbittorrent/qBittorrent (github.com)\n需要在配置文件qBittorrent.conf加一行（注意需要通过映射目录做好配置的持久化）\n1 WebUI\\HostHeaderValidation=false 重启容器，然后访问页面，使用默认账户admin，密码adminadmin登陆\n私有云盘 这里可以选择seafile和nextcloud，或者直接搭一个webdav服务器（可以参考最后的docker-compose配置），不过测试了下性能不太好。\n最终效果 做好端口映射后公网就可以访问网页：\n我在这里顺手为域名去加了证书来支持https访问\n直接访问网页是最方便的方式，但是受限与浏览器的原因，Chromium系的浏览器不能使用hevc的硬解，win10下的Edge应该可以通过额外的插件来实现hevc硬解，因此，想要通过客户端硬解来节省可怜的小水管带宽的话，可以使用Jellyfin的客户端：jellyfin/jellyfin-media-player: Jellyfin Desktop Client based on Plex Media Player (github.com)，或者用safari等浏览器\n最好提前用tmm等工具搜刮一下相关的介绍/图片补充元数据，然后统一重命名（为了不妨碍做种，我选择用cp -al硬连接到Jellyfin的目录，再改名，这样也不会占用额外的空间）：\n可以看到Jellyfin演员配图质量还是挺高的：\n转码速度还是挺快的，瓶颈主要是在家里的上行带宽只有可怜的10m多，所以网页端转码，最多只能开到1080p，码率也不敢太高更不可能多人一起看了， 而如果用客户端或者safari浏览器硬解，4K direct几乎可以十分流畅的播放：\ndocker compose配置参考 最后附一个docker-compose的配置参考：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 version: \u0026#34;3.9\u0026#34; services: jellyfin: image: linuxserver/jellyfin:10.7.7 container_name: jellyfin ports: - 8096:8096 # web ui - 8920:8920 # https - 7359:7359/udp - 1900:1900/udp group_add: - 109 # add to render group, see: cat /etc/group | grep render environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai volumes: - /data/config/jellyfin:/config # 配置 - jfcache:/cache - /data/share/media:/data - /data/share/pt/qbittorrent/downloads/:/data/downloads:ro - /data/cert:/cert devices: - /dev/dri:/dev/dri # Intel 集显驱动 restart: unless-stopped qbittorrent: image: linuxserver/qbittorrent:4.4.0 container_name: qbittorrent ports: - 6881:6881 - 6881:6881/udp - 8080:8080 # web ui environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai - WEBUI_PORT=8080 volumes: - /data/config/qbittorrent:/config - /data/share/pt/qbittorrent/downloads:/downloads - /data/cert:/cert restart: unless-stopped webdav: # webdav服务器，外网使用性能不太好 image: hacdias/webdav:v4.1.1 container_name: webdav ports: - 8081:80 environment: - TZ=Asia/Shanghai command: [ \u0026#34;--config\u0026#34;, \u0026#34;/opt/webdav.config.yml\u0026#34; ] volumes: - /data/share:/data - /data/config/webdav/webdav.config.yml:/opt/webdav.config.yml - /data/cert:/cert restart: unless-stopped tmm: # 一个影音刮削软件，可以直接部署到服务器上通过vnr远程使用 image: tinymediamanager/tinymediamanager:4.2.5.1 container_name: tmm ports: - 4000:4000 environment: - USER_ID=1000 - GROUP_ID=1000 - TZ=Asia/Shanghai volumes: - /data/config/tinymediamanager:/data - /data/share/media:/media - /data/cert:/cert restart: unless-stopped volumes: jfcache: ","date":"2022-01-17T16:35:05+08:00","image":"https://blog.kagaya.fun/p/2022/first-nas/59474391_p0_hu_cba447be72a503a9.jpeg","permalink":"https://blog.kagaya.fun/p/2022/first-nas/","title":"瞎折腾：利用NUC11打造年轻人的第一台NAS与个人影音中心"},{"content":"打开我的bangumi简单回顾一下2021年年初看过的动漫：如果算上从去年秋季开播的半年番和剧场版，总共看了15部，其中只有一部剧场版，即前几天刚上映的《绯红的子弹》。这个追番量对我个人来说其实还蛮多的，毕竟平时学校实验室的事情也比较短，工作日的话基本只有晚上回家才能看一两集，因此只有一部分番剧让我可以保持足够的兴趣在刚刚更新的时候去看，有一部分番剧只能等冬季番结束，春季番开始但没有完全开始的这个空档补回来。后面简单回顾总结一下看过的冬季番，特别是安利一下我比较喜欢的番剧。\nTL;DR 推荐：\n摇曳露营∆ 奇蛋物语 无职转生～在异世界认真地活下去～ 弱势角色友崎君 工作细胞Black 进击的巨人最终季 上半 咒术回战 还不错：\n工作细胞！！ 环太平洋：黑色禁区 五等分的新娘∫∫ 堀与宫村 Re:0 第二季后半 剧偶像 不推荐：\n名侦探柯南 绯红的子弹 搁置：\n怪物事变 避雷：\n回复术士的重来人生 安利推荐 摇曳露营∆ 尽管久闻摇曳露营的大名，但是一直以为就是个普普通通的萌豚番，没什么动力去看。本来也是没打算看冬季更新的第二季的，因为完全没看过前作，而且一直以来在B站追番的习惯对于阿b没有买的番一般也很少关注（这让我错过了不少优秀的作品）。不过在友人A的强烈安利下抱着试一试的心态去看了第一集，确实是一部值得一看的作品，真香。\n摇曳露营的原作虽然也是方文社，我对于方文社的印象一直停留在《New Game》以及《点兔》这类萌豚治愈向作品，当然也有像《学园孤岛》这种剑走偏锋的作品。这些都是很好的作品，特别是点兔也不再局限于描述少女们美好友情的萌萌日常生活，开始触及到一些诸如亲情爱情之类的话题，丰富人物形象的同时也让剧情也显得不再单薄。\n说回摇曳露营，总体而言它的剧情就显得十分单调或者说根本没有什么剧情。如果说它是单元剧的形式那么她的剧情结构也十分简单：JK们或组团或独自去露营，看风景，然后回家。所以它更像是一部旅游宣传片或者旅行Vlog，但是就是这样一部剧情简单的番，却有自己独特的魅力能吸引看番的观众，在Bangumi上斩获8.3分的高分。\n每周看摇曳露营的时光总是让人觉得短暂而宝贵，因为在这一段时间内，难得可以放下手中的事情，平静下来，抛开繁琐的事情去另一个地方来一段短暂的旅行。这一季的op和ed都给我留下了非常深刻的印象。特别是这一季的ed《はるのとなり》，每次ed悄悄响起的时候，就是用音乐来给一段旅途画上句号，该说告别的时候了，或许有些许不舍与寂寞，但更多的是期待一下次的相见。\n「ただいま」/ 我回来了\nゆっくりおやすみ / 轻轻的说一声晚安\n少し寂しくて暖かい / 虽然会有些寂寞但仍然温馨\n旅の終わり / 旅途的结束\n——《はるのとなり》\n就是这么神奇，或者说如果对上电波的话，它能让如今快节奏的生活来一点悠哉悠哉的调味品，让整个人能够沉静下来，和主角们一起旅行，感受生活。\n人活着就要多看露营，只要看露营人就会感到幸福\n摇曳露营确实描绘了一种理想的远离城市喧嚣的露营生活，甚至让我有了约朋友们一起出门去露营的想法，也和朋友一起去转了五角场的迪卡侬看了下露营装备的价格。碰巧B站上发现了一位露营up主，可以先云体验一下，等之后有条件了，一定实现一下这个梦想。\n奇蛋物语（Wonder Egg Priority） 这应该是这一季最给我最大惊喜的番剧了\n惊喜主要来自于两个方面：\n脚本：野岛伸司 动画制作：CloverWorks 也就是故事的剧情和动画制作，先说看完整部作品的剧情方面的感受。剧情脚本主要由活跃于日本TV电视剧界的著名编剧野岛伸司1负责，这一次也是他第一次尝试跨界来做动画，本身脚本实力毋庸置疑。\n野岛伸司，日本知名编剧，新潟县柏崎市出身。活跃领域亦包括诗、绘本、小说、作词等方面。曾执笔多部脍炙人口的电视剧，如《101次求婚》、《高校教师》、《同一屋檐下》、《人间失格》、《圣者的行进》等剧，为90年代的社会写实派路线代表性人物，其企画的《沒有家的女孩》和担任原案的《新白色之恋》亦造成话题。\n野岛本身的风格也十分独特，非常喜欢在作品表现一些社会问题，切入社会的阴暗面，诸如校园暴力、自残、师生恋等等，也有著名的“TBS野岛伸司黑暗四部曲”。这次《奇蛋》的剧情也是野岛擅长的对未成年人自杀问题的讨论。\n14岁少女·大户爱在深夜散步的途中，随着神秘声音的指引，获得了一枚「蛋」。\n「如果想要改变世界的话」 「那么就在现在做出选择」 「请相信自己——」 「打破这枚蛋——」\n而在打破「蛋」之后，等待着爱的是……。\n故事中的四位主角都是因为失去了对自己至关重要的人（自杀）通过「奇蛋」而相遇，每一枚「蛋」都是对应着一位因各种原因自杀的少女，主角们需要通过打破「蛋」来进入「蛋」中少女的世界，通过揭露背后的原因以及战斗，将其从自杀的阴影中解脱出来。每一次打破「蛋」都是一次挑战或者任务，当主角累计完成足够的任务之后，就可以让那位对自己重要的人复活。\n整个故事前期是以主角进入一枚枚「蛋」来作为单元剧展开的，每一个单元剧都会涉及一个新人物的故事，并通过主角们来挖掘背后的牵扯的复杂感情瓜葛，揭露一些不为人知的阴暗角落。同时后期会有一些神奇的展开，交代「蛋」的来源以及与其紧密相关的两位神秘人「表帐」「里账」的背景，让剧情层次再上一层。\n动画制作方面是《奇蛋》的另一个亮点，与相对阴暗的故事内容相比，画面风格却是十分鲜艳明亮的，让人甚至有一种错觉会认为剧情是十分轻松愉快的。我认为野岛也是选择这样一种动画独特的表达形式之前早就已经做好的决定，光鲜艳丽的画面背后，往往是不为人知的黑暗面。就像一把尖刀划破了这表面的虚伪，正正好好刺中了背后的邪恶与丑陋，给观众强烈的视觉与心灵震感。\n而在这样的背景下，四位不同身世的主角要做的事情就是打破奇蛋去战斗，和黑暗、消极战斗，和自己内心的阴暗情绪战斗，拯救需要帮助的人并最终拯救自己，这也是这部番要表达的主题：打破现状，勇于斗争。\n勇于对抗生活的消极、乐观生活的处事态度——苦恼的事情，烦恼的瞬间，都会像汽水中的气泡一样，稍纵即逝。\n人设，战斗部分也是看点之一，导演若林信很好把握住了武戏与文戏之间的节奏，战斗部分酣畅淋漓，紧张刺激，特别是女主那句“我已经怒不可遏了！”更像是传统战斗番最后用来结束战斗的必杀技宣言，为一场战斗画上句号。要知道女主大户爱本身也是因为在学校被孤立，拒绝上学，后来因为唯一的朋友自杀而深受打击，并变得越来越封闭自己，当随着剧情的发展，本来最需要他人帮助的爱，为了他人挺身而出，通过这句台词宣告了自己可以战胜内心的脆弱，逐渐成长。\n整体看下来剧情方面还是很吸引人的，非常佩服野岛对于剧情铺垫与情感发展的掌握能力，故事中有非常多的人物对白，同时与战斗环节相辅相成，既不会让对白显得枯燥，也不会让战斗显得空洞，总之十分推荐。\n无职转生 “我要在这个异世界拿出真本事！”\n《无职转生》这部番可以说是上一季中最大的争议之一了，但是产生这样的争议其实并不奇怪，完全是因为小众圈子与主流文化观念产生矛盾和冲突引起的，这么多年了类似的甚至更过分的深夜动画也没有像这样引起b站的舆论危机，可以说负有管理和推广责任的b站应该负有主要责任。对于一直强调自己社区属性的b站来说，之后只会有更多类似的问题，如何处理不同社区文化之间，社区文化与主流文化之间的矛盾，做好社区经营管理者，对于b站来说是破圈发展之后必须要面对的问题。\n有点跑题了，说回到《无职转生》，先说结论，作为一部动画来说来看，无职确实可以算上是一部优秀的作品。异世界转生系列作品近几年实在是太多了，很难再有给人眼前一亮的作品出现了，所以我本来并没有抱有太大期待，直到我看到了下面这张gif：\n无职从宣传片中就表现出来极高的制作水准，如果知道一些动画制作原理的都知道，对于这种动作丰富，场景快速变化的镜头，如果要做到像上面这样足够的帧数动作人物不走形，对于原画张数的要求是十分高的。如果按照24帧计算，每一秒需要24卡，如果2卡需要一张原画，那么一秒也要12张原画，如果镜头稍微长一点那么原画数量直接爆炸。这一点可以参考巨人前三季里所有立体机动镜头的制作，基本都要靠今井有文这样的dalao来顶着，才能在质和量上都有保证。目前动画制作还有一部分工作类似于传统的手工业作坊，极为的依赖人的技术和经验而不能完全的数字化，因此对于一些制作精良的动画，背后必然是一群充满热爱的动画人为之付出了努力和汗水。\n更为有意思的是，为了制作无职甚至WHITE FOX和EGG FIRM共同出资成立了一家新公司Studio Bind，可见出资方对于这部动画的重视，导演岡本学似乎还是个新人，不过从结果来看，属实有点厉害了，天才新人？无职小说本身也是类似国内起点小说常年霸榜的作品，描述了男主转生到异世界后经历了一位伟大魔法师一生的故事，有消息说之后可能会做成长篇动画，如果之后还能保持这样的节奏和制作水准，还是非常值得期待的。\n总结 除了上面提到的三部非常喜欢的作品，《工作细胞black》《咒术回战》《弱势角色友崎君》还有mappa的《巨人最终季》也是个人看来很优秀的作品，当然上一季还有一些口碑很好的作品没有来得及看，比如《赛马娘2》，之后有机会一定补一下。\n之后b站的番剧要开始先审后播了，当然我认为这样的制度建设和规范是有意义的，对于动画制作方来说如果想要在大陆市场获得收益就必须提前安排制作进程，各方反馈看来这会大大改善原来紧张的制作排期，改善制作人员的待遇，某种意义上也是好事，但是更重要的是需要落实透明的规则标准，诸君且看且珍惜吧。\n野島伸司 | Bangumi 番组计划\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2021-04-30T00:00:00Z","image":"https://blog.kagaya.fun/p/2021/bangumi-in-2021-spring/87505047_p0_hu_7cd659b860053354.jpg","permalink":"https://blog.kagaya.fun/p/2021/bangumi-in-2021-spring/","title":"2021春季看番总结与推荐"},{"content":"从今天开始打算尝试开一个系列的坑，分享一些Golang学习过程中觉得有趣的设计，系列的名字就打算叫Amazing Go。这个系列理论上并不打算作为一个语言教程，你可以单纯把它理解为一个Golang爱好者为了向别人介绍这门语言的优秀设计，也可以看作是对于Golang的某一项具体特性所做的原理剖析。我希望在每一篇文章中在介绍基础语法的基础上，挖掘背后的设计思路和原理，又或者从开发者的角度提供一些新的思考。\n希望能对你有所帮助。\n本篇就是这个系列的第一篇，我打算从我最喜欢的Golang特性，也是众多gopher津津乐道的Go特性——Interface说起。\nWhy Interface? 在介绍Golang的接口语法之前，我们先简单回顾一下什么是接口。IOP（Interface Oriented Programing），即面向接口编程，尽管还是属于OOP面向对象编程的范畴，这也是**多态(polymorphism)**的重要实现手段，但我觉得还是有必要单独拿出来说。现在复杂的软件系统不是由一个人单独完成的，软件设计的工作往往会涉及到一个团队之间的沟通协作，我们也总希望软件具有良好的可扩展性和可维护性。同时，在敏捷软件开发过程中，软件拥有了生命力，能够快速迭代推陈出新。因此，我们希望通过一种方法来隐藏软件设计中的部分复杂度，对复杂的部分进行良好的封装来降低协作成本，提高开发效率，于是我们引入了接口（Interface）这一概念：\nIn computing, an interface is a shared boundary across which two or more separate components of a computer system exchange information.1\n接口的作用其实就是为不同层级的模块提供了一个定义好的中间层，上游不再需要依赖下游的具体实现，充分地对上下游进行了解耦2。\n我认为接口是人类工程学中的一次伟大创造，有了接口，我们不再需要了解接口背后的实现方法，大大简化了使用门槛：就如同我们使用插座不需要了解火线和零线是如何通过交流电把能量传递到用电器中，在这里，插座就是一种接口，可以方便给所有符合接口标准的用电器提供电能，接口的好处不再赘述，下面会先简单介绍一下C++和Java等传统OOP语言的接口设计，并在下一节中着重介绍Golang中的接口设计。\nC++ Interface 我们先简单复习一下C++，接口在C++中尽管没有专门的关键字，但是一般使用虚函数或纯虚函数来声明一个抽象类来表示接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Pat { public: string name; // 纯虚函数，要求派生类必须实现 virtual void bark() = 0; // 虚函数，提供默认实现 virtual string getName() { return name; } }; class Cat : public Pat { public: void bark() { cout \u0026lt;\u0026lt; \u0026#34;喵喵喵\u0026#34; \u0026lt;\u0026lt; endl; } }; class Dog : public Pat { public: void bark() { cout \u0026lt;\u0026lt; \u0026#34;汪汪汪\u0026#34; \u0026lt;\u0026lt; endl; } string getName() { return \u0026#34;this dog\u0026#39;s name is \u0026#34; + this-\u0026gt;name; } }; 我们定义了一个基础类Pat，它包含两个虚函数（其中一个为纯虚函数），我们分别通过Cat类和Dog类来继承Pat类并实现对应的函数，从而通过Pat类的指针调用派生子类的方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int main() { Cat c; Dog d; c.name = \u0026#34;Tom\u0026#34;; d.name = \u0026#34;Spike\u0026#34;; Pat *p = \u0026amp;c; p-\u0026gt;bark(); cout \u0026lt;\u0026lt; p-\u0026gt;getName() \u0026lt;\u0026lt; endl; p = \u0026amp;d; p-\u0026gt;bark(); cout \u0026lt;\u0026lt; p-\u0026gt;getName() \u0026lt;\u0026lt; endl; return 0; } 输出结果：\n这里本质上就是实现了一个接口Pat，对于使用者来说，不需要去关心Pat到底是Dog还是Cat，只需要知道它有两个方法，bark和getName，其中bark对于派生类来说必须有自己的实现方式，而getName提供了默认的实现，即便Cat类没有定义这个方法，但仍然可以可以通过基类指针调用并输出。对外只需提供Pat接口，隐藏了内部具体的实现细节。\nC++没有专门的Interface关键字，通过提供虚函数来实现接口特性，一个类只能继承一个虚函数，同时额外区分了提供默认实现和强制子类实现的函数，不含纯虚函数的抽象类是可以被实例化的，这是非常不合理的。除此之外，理论上接口应该只关注操作方法，作为接口的基类函数却可以定义自己的公有和私有变量，造成一定的耦合，对于接口的实现来说往往会带来一些麻烦。\n另外，作为动态语言的Python，也是通过抽象类来实现类似接口的功能。\nJava Interface Java继承了C++的抽象类定义的同时，还专门提供了interface关键字，都是用来对对象方法提供抽象的接口。\n抽象类中的方法可以有方法体，就是能实现方法的具体功能，但是接口中的方法不行。 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是 public static final 类型的。 接口中不能含有静态代码块以及静态方法(用 static 修饰的方法)，而抽象类是可以有静态代码块和静态方法。（jdk1.8后可以接口可以包含静态方法和实现） 一个类只能继承一个抽象类，而一个类却可以实现多个接口。 Java中完善了C++中抽象类的一些问题（比如可以实例化）并单独提供了Interface关键字来针对接口的抽象，但是在实际使用时，往往优先使用接口而不是抽象类，这可能会给开发人员带来一些困扰。\n抽象类的例子与C++类似，我们还是看一下上面的例子，使用Java interface的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 // Pat.java package demo; public interface Pat { // 接口中可以声明变量，默认为 public static final String name = \u0026#34;pat\u0026#34;; // 默认为 public abstract void bark(); String getName(); // 接口中可以实现静态函数 static String getPatName() { return name; } } // Cat.java package demo; public class Cat implements Pat { public String name; public Cat(String name) { this.name = name; } public void bark() { System.out.println(\u0026#34;喵喵喵\u0026#34;); } public String getName() { return name; } } // Dog.java package demo; public class Dog implements Pat { public String name; public Dog(String name) { this.name = name; } public void bark() { System.out.println(\u0026#34;汪汪汪\u0026#34;); } public String getName() { return \u0026#34;this dog\u0026#39;s name is \u0026#34; + name; } } // demo.java package demo; import java.lang.*; public class demo { public static void main(String[] args) { Cat c = new Cat(\u0026#34;Tom\u0026#34;); Dog d = new Dog(\u0026#34;Spike\u0026#34;); Pat p = c; p.bark(); System.out.println(p.getName()); p = d; p.bark(); System.out.println(p.getName()); } } 输出同上，可以看到Java借助interface关键字轻松实现多态，并且interface有着抽象类所没有的严格限制，比如只能声明抽象函数，进一步分离了对象的方法声明和方法实现，实现接口也需要使用implements关键字显式声明，可以同时声明实现多个接口。\n特别的是，Java中的接口仍然选择保留声明变量，但声明变量和实现函数必须是静态的，变量还限制为final，且方法不能通过实现类来重写和调用，即不能提供方法的默认实现。默认实现仍然需要借助抽象类来提供，个人觉得这还是有一些复杂，我们再做抽象时不得不考虑我们应该使用抽象类还是使用接口。\nInterface in Go Go作为一门年轻的高级语言，自然也吸取了C++和Java中优秀的语法设计，同时以围绕简单直接的设计哲学，Go对于接口的设计也形成了自己独特的风格。\nGo语言的设计哲学之一就是简单直接，少即是多，作为一门现代编程语言，他甚至抛弃了OOP中关键的class关键字，转而去拥抱类似C中的struct。但和C这种纯过程式的语言不同，Go依然拥有面向对象的特性，让面向对象的软件设计方法依然在Golang中发光发热。\n另外补充一下，Go由于同时抛弃了public、private、protected等关键字，对于公有和私有方法/变量的处理是通过名称首字母的大小写来区分，大写字母开头即为公有方法/变量，小写字母开头则为私有方法/变量。\n说回到接口，在Go中，也与Java一样提供了interface关键字，通过interface来声明一个接口：\n1 2 3 4 type Pet interface { Bark() GetName() string } Go中的接口声明显得十分直观简单，只包含接口名+包含的方法，不允许声明变量，不允许有方法实现，彻底将对象中的方法抽象了出来，没有一点多余的要素，只告诉使用方这个接口包含哪些方法可以使用。\n更加令人惊喜的是，Go中的接口实现方法采用了类似鸭子类型的实现策略。\nDuck Typing 鸭子类型（Duck Typing）3是一种动态类型语言的设计风格：\nIf it walks like a duck and it quacks like a duck, then it must be a duck\n鸭子类型在Python等动态类型的语言中得到了非常广泛的应用，在鸭子类型中，关注点在于对象的行为，能作什么，而不是关注对象所属的类型。这样，我们就可以在不继承对象的情况下实现了多态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Duck: def quack(self): print \u0026#34;这鸭子正在嘎嘎叫\u0026#34; def feathers(self): print \u0026#34;这鸭子拥有白色和灰色的羽毛\u0026#34; class Person: def quack(self): print \u0026#34;这人正在模仿鸭子\u0026#34; def feathers(self): print \u0026#34;这人在地上拿起1根羽毛然后给其他人看\u0026#34; def in_the_forest(duck): duck.quack() duck.feathers() def game(): donald = Duck() john = Person() in_the_forest(donald) in_the_forest(john) game() 如上的python代码中，in_the_forest方法的形参并不是一个被继承的父类对象类型，而是假定duck参数包含quack与feathers两个方法，或只要包含着两个方法的对象，就认为是合法的参数，解耦了方法的实现和声明。\n这其实与接口的思想不谋而合，但有区别的是，实现方也不用显示定义了实现了哪些接口，调用方也不关心谁实现了这些方法。这其实也导致了很多批评的声音：\nDuck Typing需要动态类型的支持，使得将可能的错误发生转移到了运行时，并且难以通过静态分析做类型推断，反而增加了程序员的负担。 “如果它走起来像鸭子并且叫起来像鸭子”，它也可以是一只正在模仿鸭子的龙。尽管它们可以模仿鸭子，但也许你不总是想让龙进入池塘。 当然这不妨碍仍有很多人喜欢Duck Typing，尤其是众多Gopher。\nWhat\u0026rsquo;s different in go 前面也说了，Golang的接口提供了类似鸭子类型的特性，为什么说类似呢，一个最重要的不同就是Golang作为一门静态类型的语言却实现了类似动态语言的特性，我们还是看上面猫和狗的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type Cat struct { name string } func (c Cat) Bark() { fmt.Println(\u0026#34;喵喵喵\u0026#34;) } func (c *Cat) GetName() string { return c.name } type Dog struct { name string } func (d Dog) Bark() { fmt.Println(\u0026#34;汪汪汪\u0026#34;) } func (d Dog) GetName() string { return \u0026#34;this dog\u0026#39;s name is \u0026#34; + d.name } 如果没有上文，我相信没有人知道这里的Dog和Cat已经实现了前文定义的Pat接口，这就是Duck Typing的魅力。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func main() { var c Cat var d Dog c.name = \u0026#34;Tom\u0026#34; d.name = \u0026#34;Spike\u0026#34; var p Pat // 接口类型 p = \u0026amp;c p.Bark() // 喵喵喵 fmt.Println(p.GetName()) // Tom p = d p.Bark() // 汪汪汪 fmt.Println(p.GetName()) // this dog\u0026#39;s name is Spike } 由于Go是一门静态类型的语言，意味着可以通过静态类型检查让你你可以放心的使用鸭子类型，而避免了可能的类型错误同时还能享受鸭子类型带来的简洁高效的语法。\n有读者可能已经注意到了，我在接口类型变量p赋值时，第一次使用了指针，而第二次原变量，这里涉及到了指针结构体和原结构体所包含方法列表的范围，有兴趣的读者可以思考一下为什么。\nOne more case 刚才这个例子或许也还不足以体现Go Interface的神奇之处，那么再来看看这个例子。\nGolang诞生至今，最成功的接口定义之一就是io.Writer和io.Reader接口：\n1 2 3 4 5 6 7 type Writer interface { Write(p []byte) (n int, err error) } type Reader interface { Read(p []byte) (n int, err error) } 两个接口分别完美继承了Unix中一切皆文件的思想，对数据源中的数据操作做了良好的抽象，任何数据源只要实现了这两个接口，就可以统一操作方法，方便对接任何调用方4：\n字符串\n1 2 r := strings.NewReader(\u0026#34;hello, go\u0026#34;) r.Read(...) 字节序列\n1 2 r := bytes.NewReader([]byte(\u0026#34;hello, go\u0026#34;)) r.Read(...) 文件\n1 2 f := os.Open(\u0026#34;foo.txt\u0026#34;) // f 满足io.Reader f.Read(...) tcp socket\n1 2 r, err := net.DialTCP(\u0026#34;192.168.0.10\u0026#34;, nil, raddr *TCPAddr) (*TCPConn, error) r.Read(...) http request\n1 2 r := bytes.NewReader([]byte(\u0026#34;hello, go\u0026#34;)) req, err := http.NewRequestWithContext(ctx, \u0026#34;POST\u0026#34;, url, r) 读取压缩文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { f, err := os.Open(\u0026#34;hello.txt.gz\u0026#34;) if err != nil { log.Fatal(err) } zr, err := gzip.NewReader(f) if err != nil { log.Fatal(err) } if _, err := io.Copy(os.Stdout, zr); err != nil { // 读取到stdout log.Fatal(err) } if err := zr.Close(); err != nil { log.Fatal(err) } } Writer和Reader接口的设计十分简洁，同样继续遵循了Golang的设计哲学，Go语言lib中的大部分接口都是如此，只包含极少数的方法，同时遵循SOILD原则中的Open/Close开闭原则，即对扩展开放，对修改封闭\n如果我们想设计一个既包含read又包含write，同时还能有一个输出数据源类型的方法，那么我们可以通过接口嵌入的方式来扩展原有接口：\n1 2 3 4 5 type MyReadWriter interface { io.Reader io.Writer DataType() string } // 这个接口包含了 Read(), Write(), DataType() 三个方法 Easy to test 使用接口进行良好抽象锁带来的另一个好处就是使得代码变得易于测试，在 Go 语言中如果我们完全不使用接口，是写不出易于测试的代码的，作为静态语言的 Go，只有我们使用接口才能脱离依赖具体实现的窘境，接口的使用能够为我们带来更清晰的抽象，帮助我们思考如何对代码进行设计，也能让我们更方便地对依赖进行 Mock。\n对于上文的接口Pet，我们使用mockery来生成对应的mock\n自动生成mocks/Pat.go，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // Code generated by mockery 2.7.4. DO NOT EDIT. package mocks import mock \u0026#34;github.com/stretchr/testify/mock\u0026#34; // Pat is an autogenerated mock type for the Pat type type Pat struct { mock.Mock } // Bark provides a mock function with given fields: func (_m *Pat) Bark() { _m.Called() } // GetName provides a mock function with given fields: func (_m *Pat) GetName() string { ret := _m.Called() var r0 string if rf, ok := ret.Get(0).(func() string); ok { r0 = rf() } else { r0 = ret.Get(0).(string) } return r0 } 我们就可以方便的利用接口的特性进行单元测试，使用mock替换实际实现，构造与上下文无关的测试环境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package pat import ( \u0026#34;testing\u0026#34; \u0026#34;demo/mocks\u0026#34; ) func TestPat(t *testing.T) { mp := new(mocks.Pat) mp.On(\u0026#34;GetName\u0026#34;).Return(\u0026#34;mock name\u0026#34;) myPat := Pat(mp) // 这里模拟将mock传入给以Pat为参数的函数中来测试对应函数功能 t.Log(myPat.GetName()) // 调用Pat.GetName } 同理，我们可以模拟sql语句，http请求等，保证单元测试的上下文无关，提高单元测试的覆盖度来保证代码质量。\nBehind the Go Interface Gopher们常说Go语言写起来像动态类型的语言一样轻松，除了对于变量类型的自动推导，Interface所带来的Duck Typing特性也功不可没，这样类似动态语言的特性究竟是如何实现的呢？\n我们先来看一下常用ToString函数，它可以这么实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type Stringer interface { String() string } func ToString(any interface{}) string { if v, ok := any.(Stringer); ok { return v.String() } switch v := any.(type) { case int: return strconv.Itoa(v) case float: return strconv.Ftoa(v, \u0026#39;g\u0026#39;, -1) } return \u0026#34;???\u0026#34; } 如果接受的未知类型能断言为Stringer接口类型，则说明包含String方法直接调用，否则根据具体类型调用具体方法进行转换。\n我们可以看到这里出现了两种interface，一种出现在接口类型定义Stringer，一种作为ToString的形式参数类型，代表不包含任何方法的接口。在Go中，interface其实是一种结构体，源码中分别区分了这两种interface：eface与iface，即空接口和包含方法的接口。\n现在我们可以看到，Go中的接口其实就是期望结构体应当包含的方法集合，那么空接口自然就可以接受任意类型的变量，实现了类似泛型的功能，我们会在下一节中简单介绍将在go1.18中登场的go泛型中接口的变化，但是要强调的是，interface{}不是任意变量，而是一种变量类型，尽管它可以由任意变量转换而来。\n回到Go对于接口的实现方法，我们来定义一个Binary类型来实现上面的Stringer接口：\n1 2 3 4 5 6 7 8 9 type Binary uint64 func (i Binary) String() string { return strconv.Uitob64(i.Get(), 2) } func (i Binary) Get() uint64 { return uint64(i) } 此时我们声明一个变量为Binary类型，在内存中的结构如下（假设为32bits系统），需要占用两个32bits空间\n紧接着我们把它复制给一个Stringer接口类型或者通过强制类型转换把它转换为Stringer，那么在内存中会分配一个interface结构体来保存我们的Binary变量\n可以看到，这个interface结构体主要包含两个指针字段，其中data指针指向的内存存放了具体的数据，而tab指针指向一个itable表，包含了data部分的数据类型和保护的方法指针表。而生成这样一个itab表是可以在编译时就确定的，接口类型可能有$ni$个方法，具体类型可能有$nt$个方法，一般寻找从接口方法到具体方法的映射需要$O(ni\\times nt)$的时间复杂度，Go编译器通过对两个方法表进行分类，同时进行搜索可以优化到$O(ni+nt)$的时间复杂度5。\n从Russ Cox的博客中可以看到，Go的设计者还对interface进行了内存优化\n如果一个interface不包含任何方法，就可以用实际的类型变量存储在itab指针的位置 而对于占用不大于32bits的变量，则可以直接存放在data部分替换原来的data指针 又或者同时满足上面两种情况下 然而在实际的Golang interface实现中，只对不包含任何方法的interface做了优化，即eface\n1 2 3 4 5 // 64bit golang src/runtime/runtime2.go type eface struct { // 16 bytes _type *_type data unsafe.Pointer } 完整的interface:\n1 2 3 4 5 // 64bit golang src/runtime/runtime2.go type iface struct { // 16 bytes tab *itab data unsafe.Pointer } itab:\n1 2 3 4 5 6 7 8 9 10 11 12 // src/runtime/runtime2.go // layout of Itab known to compilers // allocated in non-garbage-collected memory // Needs to be in sync with // ../cmd/compile/internal/gc/reflect.go:/^func.dumptabs. type itab struct { inter *interfacetype _type *_type hash uint32 // copy of _type.hash. Used for type switches. _ [4]byte fun [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter. } Generic Type 在2022年初到来的Go 1.18中，会带来期待已久的范型功能，在目前的泛型提案中，interface增加了新的特性。\nType list:\n1 2 3 4 5 6 7 8 9 10 11 12 type MyC2 interface { type int, int32, int64 } func F2[T MyC2](t T) { fmt.Printf(\u0026#34;%T\\n\u0026#34;, t) } func main() { var t2 string F2(t2) // string } 拥有 type list 的 interface 仅能用于做为类型参数的约束，而不能像普通 interface 类型那样使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;fmt\u0026#34; ) type MyC3 interface { M3() type int, string, float64 } func F3[T MyC3](t T) { fmt.Printf(\u0026#34;%T\\n\u0026#34;, t) } type MyT3 string func (t3 MyT3) M3() { } func main() { t3 := MyT3(\u0026#34;hello\u0026#34;) F3(t3) // main.MyT3 } 不过目前社区有了新提案 type set来统一interface的语法，不知道当1.18泛型正式开放后会是哪种形式:\n1 2 3 4 5 6 7 8 9 10 // 当前的type list type SignedInteger interface { type int, int8, int16, int32, int64 } // type set理念下的新语法 type SignedInteger interface { ~int | ~int8 | ~int16 | ~int32 | ~int64 } 目前来说go的interface可以很好的替代一部分泛型的功能，但是并不能完全替代泛型，因此引入泛型后两者必然有一些功能的重叠部分，而且会给go简洁的语法引入一定的复杂性，破坏原有的go语言美感。不过目前还都不能盖棺定论，go团队也需要在社区收集充足的意见决定如何把泛型带入go语言中，让gopher们在享受泛型带来的便利同时，避免引入过多的复杂性，期待1.18中的泛型能把go带入一个新的阶段。\nConclusion C++只能通过虚函数来实现类似接口的功能 Java既可以使用虚函数，也有专门的interface关键字，使用时需要进行区分 Go中的interface可以实现类似动态语言鸭子类型的特性 良好的接口抽象能提高代码的可扩展性并更加易于进行单元测试 Go中的接口类型是一个结构体，包含两个指针分别指向数据和方法表，不包含方法的接口会进行内存优化 其他关于Go中类型转换，断言，动态派发的具体汇编实现可以参看draveness的这篇博客\nReference https://en.wikipedia.org/wiki/Interface_(computing)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://draveness.me/golang-101/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://zh.wikipedia.org/wiki/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gocn.vip/topics/11825\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://research.swtch.com/interfaces\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2021-04-12T00:00:00Z","image":"https://blog.kagaya.fun/p/2021/amazing-go-interface/golang_hu_42190784e75fa0e4.jpeg","permalink":"https://blog.kagaya.fun/p/2021/amazing-go-interface/","title":"Amazing Go Interface"},{"content":"简介 函数式编程（英语：functional programming）或称函数程序设计、泛函编程，在Wiki中是这么定义的：一种编程范式，它将电脑运算视为函数运算，并且避免使用程序状态以及易变对象1。比起指令式编程，函数式编程更加强调程序执行的结果而非执行的过程，倡导利用若干简单的执行单元让计算结果不断渐进，逐层推导复杂的运算，而不是设计一个复杂的执行过程。\n尽管函数式编程已经诞生了很久的时间，但是随着分布式并行计算的兴起，函数式编程独特的设计逻辑以及自诞生之初所具备的特性往往会给我们一种新的视角来看待程序设计。每当我在使用函数式的思维来编写代码时，往往感觉自己就像在做数学计算，在代码中撰写一行行公式，给编程带来一种独特的体验。\n当函数成为编程语言中的一等公民（first class citizen）时，给我们的编程思维都会带来很大的影响。当然，这会带来一些显而易见的好处，但是如同其他新技术一样，函数式的方法也存在一定的局限性。本文会以Haskell和Go两种语言为例，从函数式语言的发展讲起，紧接着讲述函数式语言在一些领域的应用（如近几年火热的Cloud Native），最后总结了目前人们对于函数式语言的优越性与局限性的看法。从以上三个方面阐释函数式程序设计的影戏那个与应用\n函数式语言的发展 Haskell Haskell是一种标准化的，通用的纯函数式编程语言，有惰性求值和强静态类型。它的命名源自美国逻辑学家哈斯凯尔·加里，他在数理逻辑方面上的工作使得函数式编程语言有了广泛的基础。在Haskell中，“函数是第一类对象”。作为一门函数编程语言，主要控制结构是函数。Haskell语言是1990年在编程语言Miranda的基础上标准化的，并且以λ演算为基础发展而来2。\n与命令式编程语言不同，Haskell是一种纯函数式编程语言。虽然它现在仍然局限在学术界和需要大量使用高等数学的行业如金融业，但是这并不意味着数值计算。\n功能编程范式依赖于编程功能 ，其行为为数学函数，允许创建不修改外部数据或具有与外部上下文的可观察的交互功能。可观察的交互是指将数据写入文件或引发异常的事情。在技术上这意味着功能没有 side-effects，这也可能成为一等公民的功能。函数和其他类型一样，也可以作为其他函数的参数。\nFP in Golang Go语言是Google开发的一种静态强类型、编译型、并发型，并具有垃圾回收功能的编程语言。Robert Griesemer、Rob Pike及肯·汤普逊于2007年9月开始设计Go，稍后Ian Lance Taylor、Russ Cox加入项目。Go是基于Inferno操作系统所开发的。Go于2009年11月正式宣布推出，成为开放源代码项目，支持Linux、macOS、Windows等操作系统。\n尽管Go不是专门设计的函数式编程语言，但其语法支持我们使用一些函数式的编程思维。只需要满足两个规则：\n没有数据赋值：数据对象赋值后不再改变 没有隐藏状态：隐藏在内部的状态在设计的使用就应该被避免。对于函数式编程来说，状态没有消失，而是变得明确而且可见。 当我们在Go中使用了函数式的设计思维，意味着：\nNo side-effect：函数和操作不应该在其函数范围外更改任何变量，函数应该将值返回给调用方，并且不影响任何外部状态，程序也更容易被理解，从而无需担心任何的“副作用”。 使用纯函数：函数应该是是幂等的，同样的入参只会得到相同的结果。一个函数的返回值应该只基于输入参数而没有任何的side-effect或依赖任何的全局变量。 Clean Code3：因为函数没有隐藏的状态，可以更好的理解方法、函数、类、以及整个项目。 在Golang中，可以通过7种方法来实现函数式的编程4，其中包含像高阶函数、柯里化、闭包函数、纯函数等语法在Go语言中都是支持的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // this is a higher-order-function that returns a function func add(x int) func(y int) int { // A function is returned here as closure // variable x is obtained from the outer scope of this method and memorized in the closure return func(y int) int { return x + y } } func main() { // we are currying the add method to create more variations var add10 = add(10) var add20 = add(20) var add30 = add(30) fmt.Println(add10(5)) // 15 fmt.Println(add20(5)) // 25 fmt.Println(add30(5)) // 35 } 但是对于函数式编程中常用的递归调用来说，由于Go语言的编译器不支持对尾递归的调用优化，只能采用纯递归的方式，导致在递归时需要考虑调用堆栈的大小，因此在编写递归程序的时候我们不得不考虑递归的调用深度，或者改写为迭代方式，防止内存爆栈导致的程序异常。\nGolang一般使用了strict/eager的计算方式（除了\u0026amp;\u0026amp;和||运算符），这种方式与函数式所推崇的延迟求值相反，会在调用时立即计算出对应的结果，对于延迟求值来说，我们可以使用一些方法来模拟，像是高阶函数的方式，也可以利用Go语言中的一些特性，如sync包和channel结构来进行模拟。\n一个高阶函数模拟的延迟求值方法，通过传入函数名而不是传入函数的调用，避免在传参时就计算出结果，而是在函数最后return后才计算对应的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func add(x int) int { fmt.Println(\u0026#34;executing add\u0026#34;) return x + x } func multiply(x int) int { fmt.Println(\u0026#34;executing multiply\u0026#34;) return x * x } func main() { fmt.Println(addOrMultiply(true, add, multiply, 4)) fmt.Println(addOrMultiply(false, add, multiply, 4)) } // This is now a higher-order-function hence evaluation of the functions are delayed in if-else func addOrMultiply(add bool, onAdd, onMultiply func(t int) int, t int) int { if add { return onAdd(t) } return onMultiply(t) } 最后，是关于函数式程序设计中的应用透明性：\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.\nGo语言中实现引用透明的方法是通过避免引用传递或指针传递的使用。虽然在Go语言中都是通过值传递的方式来传递参数，但是像切片或者Map这样的数据结构是通过指针的方式来进行传递。因此我们在使用函数式的程序设计思路来构造我们的程序时，应当注意引用透明性，避免在函数的计算结果受外部的状态影响或者影响外部的状态。\n对于Go语言来说，由函数式所带来的引用透明性可以带来很多显而易见的好处，比如我们可以缓存每一次函数调用的结果来达到空间换时间的目的，又或者幂等的函数使得函数的计算结构变得可预测。同时函数式天生的模块化设计理念可以让自底向上的程序设计变得更加容易。程序的调试也变得更加简单，由于函数都是各自独立的，他们的计算结果只依赖于各自的输入参数。\n最后一点也是十分重要的，由于Go语言设计之初是为了更好的支持高并发场景，而函数式设计的方法天生就支持并发运行的场景，因为这些方法不需要共享数据，无信号量、无监控、无锁、从而也不会导致竞争危害和死锁的发生3，与Go语言的高并发设计理念不谋而合。\n函数式语言的应用 FP in Compiler 在Go语言的编译器在生成中间代码（IR）时，采用了静态单赋值的特性（Static Single Assignment form、通常简写为SSA form或是SSA），如果中间代码具有静态单赋值的特性，那么每个变量就只会被赋值一次。尽管某些代码在循环中执行了很多次，但是单赋值只表示程序文本的静态属性而不是执行的动态属性。\n将程式码转换为SSA形式，最简单的方法，就是将每个被赋值的变数，以一个新的变数来取代，而新的变数名称则为一个带着编号的旧变数。SSA通过加一个特别的描述，称之为Φ 函式，来实现对于变量赋值来源不确定时的表示5。\n在《SSA is Functional Programming》6这篇文章中，作者提出了一种观点，认为SSA form其实是一种函数式编程，对于一次赋值，左侧是对应函数的形式参数，右侧是对应函数调用的实际参数。通过函数之间的嵌套调用，可以很清楚的表现SSA中的变量的实际作用范围。\n因此，对于SSA来说，SSA form的一个重要属性是，变量的定义决定了每次的使用（或在C函数中使用的情况下，支配使用节点的前驱节点）。 SSA的解释中通常未声明此属性，但是对SSA进行许多分析和优化是必需的-它是SSA语义的一部分。 在具有嵌套作用域的函数式程序中，此限制已明确且静态地编码到函数嵌套的结构中。 变量范围的概念有助于我们构造中间形式。\n而对于使用函数式程序设计的成语言来说，使用SSA的过程中，往往会绘制带有方框，任务，条件和控制流边缘的流程图。 这种表示法虽然容易被滥用，但通常更适合于解释思想以及直观地可视化地表现算法和状态转换。 函数式程序员可以借鉴这样一种方法，避免迷失在函数式编程的概念中。\nFP in Parallel Programming 为了提高计算机的运行速度和系统的处理能力，在总体设计和逻辑设计中广泛采用并行操作技术，使各部件并行工作。要求操作系统具有并发性及资源共享，可以采用并行程序设计的方法。\n对于命令式的语言来说，进行并行程序设计时会遇到许多困难7：\n程序员必须构思一个符合规范的并行算法 程序员需要将算法映射到编程语言提供的抽象上： 需要标识一定数量的顺序活动，成为可以被并行执行的任务或者进程 在任务之间定义接口，使之能够安全的进行数据同步和通信 任务之间任何的数据共享都需要程序员进行特殊保护 在某些系统中，程序员还需要负责将任务映射到对应处理器上，并确保可以和任何执行该任务的处理器进行通信 程序员往往不能干涉具体的调度细节，这样导致无法确定处理器下一步会执行哪一个任务，因此程序员还需要保证任务集不会产生死锁等问题 而与之相对比的函数式编程语言来说，对于解决并行程序设计有着先天的优势7：\n不需要新的语言构造来表达并行性，或者并发任务之间的同步和通信。并发是完全隐式的，如果计算机有空间和容量执行任务，那么就会动态的产生新任务 无需采用特殊措施来保护并发任务的共享数据 相比顺序编程，并发的函数式编程并不会让程序功能变得复杂。因为都使用相同的技术，也没有新的结构被添加进来。 程序结果是确定的，也就是说，额外的的因素例如调度策略并不会使得每一次执行的结果不同 并行函数式编程是一种处于青春期的技术。它的潜力是巨大的，但是在它获得广泛信誉之前，需要在实际应用中进行实践测试。幸运的是，现在可以实现支持实际应用程序的实现，并且可以预期，在未来几年中，使用函数式编程的使用将大大增加。使函数式程序具有吸引力的原因正是那些对分布式计算系统和运行时资源管理系统的大量需求。但是，随着实现的改进，函数式编程的成本应变得越来越可忽略。随着软件变得越来越复杂，其收益将越来越有价值。\nFP in Cloud 云原生是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。可以简单的将云原生理解为：云原生 = 微服务 + DevOps + 持续交付 + 容器化\n性能场景 云原生技术中最有具有代表性的技术之一，便是无服务Serverless技术。在Serverless中，不仅对基础设施进行了虚拟化，抽象化管理，而且可以按需启动和支付需要的资源（CPU、memory、网络带宽）。这样人们可以只考虑架构解决方案而不用去担心潜在的基础设施配置、供应、伸缩或管理8。\n在《Functional Programming in Serverless World》9中，作者利用AWS的Lambda Serverless服务对几种函数式语言在Serverless场景中的性能进行了测试，作者主要做了四个方面的测试：\n包大小 内存占用 执行效率 冷启动时间 结果如下：\n从这些测试中可以看出，不同的函数式语言在无服务器场景下运行情况是又较大的区别的。作者认为，对于Serverless技术不应该盲目的使用，而是更具应用场景再决定是否使用Serverless。同时，作者还建议，对于一个Serverless服务，最可靠安全的函数式语言应该是F#，因为由于各个云服务提供的Serverless服务所支持的语言不同，但对于.NET Core平台的支持都是最充足的。\n云原生应用场景 在《Functional Programming Languages in Computing Clouds》10这篇文章中，作者对于云计算场景下的函数式程序设计语言提出了两个问题：\n云计算构建中有哪些新的范式和问题？ 函数式编程语言（FPL）是否适合这些范式，它们是否提供新的方法来应对新的问题和挑战？ 作者着重讨论的Haskell语言在云计算场景下的表现，发现Haskell非常适合不需要在物理（或虚拟）系统的边界上分布应用程序的使用场景，但却不适用于需要与应用程序进行通信的基于分布式云的工作负载的开发，特别是分布式工作负载的协调。\n但随着一些在分布式环境中容纳不确定性的机制发展，从而使应用程序具有一定的抗脆弱性，Haskell在未来可能有资格成为合适的工具，而不是坚持严格的确定性，使得Haskell应用程序只能可以在本地系统上或通过阻塞的通信机制来保证确定性。Haskell的并行化功能是为单个系统设计的，因此未来的方向可能有一下几点：\n在分布式环境中使用轻便且健壮的通信协议 研究功能与第三方通信服务的集成（如etcd） 研究FPL在Serverless计算和IoT（那些现在没有使用FPL）中的潜在作用 同时，作者也讨论了Serverless技术带来的影响：Serverless计算与IoT通常不需要节点间的消息传递和协调，往往比起云计算更适合用函数式编程语言来实现。\nServerless技术中的代表，AWS Lambda，是一个运行代码的完全托管环境，通过将代码部署在lambda函数中，从受支持的AWS服务之一接受事件时触发我们的代码，并将事件作为参数传递。而函数式设计的程序，是将程序定义问函数，AWS Lambda提供了一个方便的抽象赖于其他的AWS服务进行交互，所有的一切都是我们作为参数接受的事件。通过函数组合的方式解决原来函数过于庞大复杂难以维护的问题，各个功能拆分为较小的函数，然后通过组合的方式实现最终的功能。遵循这种方法的优点是我们可以以可重用的方式考虑我们较小的功能。 然后，我们可以像将它们与积木一起玩时那样将它们放在一起。\n比如说，对于一个验证函数11，为了通过验证，我们需要做很多项不同的检查：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 const queryParamsIsNull = (event: APIGatewayEvent) =\u0026gt; { if (event.queryStringParameters !== null) { throw new ApplicationError( \u0026#39;Error parsing request query params\u0026#39;, [\u0026#39;Query params should be empty\u0026#39;], StatusCodes.BAD_REQUEST ); } return event; }; const pathParamsIsNull = (event: APIGatewayEvent) =\u0026gt; { if (event.pathParameters !== null) { throw new ApplicationError( \u0026#39;Error parsing request path params\u0026#39;, [\u0026#39;Path params should be empty\u0026#39;], StatusCodes.BAD_REQUEST ); } return event; }; const asUserPostEvent = (event: APIGatewayEvent) =\u0026gt; { try { const parsedBody = event.body ? JSON.parse(event.body) : {}; return { ...event, body: parsedBody }; } catch (error) { throw new ApplicationError( \u0026#39;Error parsing request body\u0026#39;, [\u0026#39;Invalid JSON\u0026#39;], StatusCodes.BAD_REQUEST ); } }; 通过函数组合的方式，我们可以很方便的决定需要做哪些检查项：\n1 const validateCreatePostEvent = compose(asUserPostEvent, queryParamsIsNull, pathParamsIsNull); 分布式系统开发 在另一片文章《Using Functional Programming for Development of Distributed, Cloud and Web Applications in F#》12，作者讨论了分布式环境下对于函数式编程语言F#的使用。\n分布式系统的开发本质上非常复杂，部分原因是在客户端和服务器端使用了不同的语言和技术，而且这些技术之间通常没有透明的无缝集成。 因此，开发人员必须将整个系统视为许多互操作的模块，始终牢记这些模块如何协同工作并交换信息。 这也使开发更加复杂，因为我们需要在源代码树中分离服务器端和客户端代码。\n作者强调了F＃非常适合于分布式云计算。 由于函数样式迫使我们以一种由无状态函数组成的方式分解问题，无状态函数仅在给定初始数据的情况下计算结果，因此我们自然可以在集群中的不同节点上并行运行那些独立的函数。 任务的分配仍然是开发人员的责任，但是功能分解过程的整体性质使其变得更加容易。\nFP in Programming Design: Optional Functions 在使用面向对象的方法编写程序的过程中，我们往往会遇到一些配置相关的问题，比如如果我们要启动一个服务器，需要为这个服务器配置各种参数，类似这样\n1 2 3 4 5 6 7 8 type Server struct { Addr string Port int Protocol string Timeout time.Duration MaxConns int TLS *tls.Config } 而往往我们不需要配置所有的参数，希望某些配置项使用默认的参数。OOP的语言中我们一般可以使用函数重载的方式解决这一问题。但是在对于先Golang这种不支持函数重载的语言，则需要定义许多构造函数，通过带不同形式参数的方式来应对不同的配置需求，又或者通过一个自定义的配置结构体，将配置项打包为一个数据结构整体传入。这样的方法虽然可以解决问题，但是从程序设计和程序可读性的角度来说，上面的做法既不高效，也不具有很好的可读性。\n前贝尔实验室成员，UTF-8与Golang的创始人之一Rob Pike曾在2014年的一篇博文《Self-referential functions and the design of options》13中，用函数式编程的的思想巧妙的提出了一种解决程序设计中进行对象参数设置的方法，展现了函数式编程在与面向对象编程中结合的魅力。\nRob Pike同样遇到了这样的问题，在他与社区的成员进行讨论后，总结并提出了Optional Function（配置函数）的方法，非常巧妙的解决了自定义配置的同时，也让代码有了自解释性，大大提高了代码的可读性。\n什么是Optional Functional，首先我们会定义一种新类型：\n1 type option func(*Server) 我们将一个入参为对象指针，无返回值的函数定义为类型option，紧接着，我们可以在对象中定义一个用于设置参数的函数Option\n1 2 3 4 5 6 // Option sets the options specified. func (s *Server) Option(opts ...option) { for _, opt := range opts { opt(s) } } 最后，我们可以定义某一个配置项的设置函数，与以往OOP中的set函数不同的是，这个函数接受一个我们想要设置的数值，返回的却是一个Option函数\n1 2 3 4 5 6 // Verbosity sets Foo\u0026#39;s verbosity level to v. func Verbosity(v int) option { return func(s *Server) { s.verbosity = v } } 这样，我们就可以通过类似下面这样的方式来方便的设置我们需要的自定义配置项，在不破坏对象封装的情况下也可以通过函数名让配置方法具有来很好的可读性:\n1 server.Option(Verbosity(3), Overtime(10), Addr(\u0026#34;192.168.10.1\u0026#34;), Port(80)) 当然，还有一些进阶的用法，比如将Option的定义修改为\n1 2 3 4 5 // 返回一个值 type option func(*Foo) interface{} // 返回option函数 type option func(*Foo) option 第一种方式我们可以通过返回的interface{}获取修改之前的配置值，第二种方法则可以返回一个包含了原始值的Option函数，通过调用该Option函数，我们可以直接将配置项恢复为原始值。结合Golang中的defer函数，便可以方便的实现对于配置项的临时修改和恢复。\nPros and Cons of FP 正如任何技术方法一样，在新技术刚刚出现时总会受到人们的热情追捧，但是随着时间的推移，慢慢人们也会发现许多问题。函数式程序设计方法也是如此，对程序员来说，函数式编程为我们程序设计带来的新的方法和思维，这些往往给一些软件设计中的难点和痛点提供了新的解决方法。同样函数式编程不是银色子弹，它也有许多自己的局限性，我们应该辩证地去看待它对程序设计的影响。接下来，我会从正面和反面两个角度总结如今人们是如何看待函数式程序设计的，以及FP所具有的优越性和局限性。\nAdvantages 函数式编程的有点是非常显而易见的，从不同角度来说，函数式程序设计的优越性往往有以下几个方面：\n函数式程序设计是非常安全的，尤其是在并行和分布式编程中，因为纯函数是天生线程安全的14 纯函数容易理解，不需要关注任何可能被改变的隐藏状态。这使得代码更短，从而保证可以容忍的错误数量更少15 函数式编程完成的程序往往更加容易测试，因为它不需要去模拟和考虑任何的Side-effect14 同时函数式使得人们更容易区分哪些函数是不纯的，因此在QA阶段可以将大部分测试资源都集中在这里，从而提高程序的可靠性14 函数式的方法更容易组合，程序设计者可以轻松的掌握一个函数的输入输出之间的关系，同时也不需要使用锁来决定函数之间的执行顺序14 函数式程序设计有着强大的抽象能力，他可以利用引用透明性（Referential transparency）来对概念进行抽象化，从而隐藏其中的复杂性。比如函数式中往往会模糊一个纯数值和函数调用结果之间的界限，程序设计者只需要知道该值是什么，而不用太关注其中的细节16 另外，函数式的程序设计方法往往可以加快程序的开发速度。我们可以运用函数式的思维将一个大的问题分解为更小的可组合的操作，然后通过组合的方式串行执行每个操作，从而使问题的解决变得更加容易。16 正如前一节提到的Functional Options在Golang中的应用，函数式的方法可以很好的提升代码的可读性。函数式设计出来的方法往往可以拥有自解释性（self-documented），方法的功能和行为等内容都定义在了输入参数或函数名当中，不需要额外的注释就可以让阅读者很好的理解代码的功能和预测代码运行的结果11 函数式程序设计中对于纯函数的使用，使得方法的计算结果变得可缓存。因为纯函数的输出只取决于函数的输入参数，那么我们便可以预先计算并缓存一些计算结果，这样在之后遇到相同的输入参数时，我们就可以直接使用缓存的结果，从而提高程序的执行效率11 Disadvantages 函数式程序设计的缺点又是会很明显，但部分缺点会隐藏在一些我们不会去注意到的地方，辩证地看待一种新技术，自然也要充分了解它的局限性，才能更好的使用它：\n首先是资源占用和运行效率方面：\n函数式设计的程序往往会比命令式的程序在执行效率上更慢，大多数FP使用特殊的不可变数据结构。如FP通过调用set函数设置一个新值，返回一个设置来新值的新数据结构而不是在原来的数据结构上进行修改14 同时，使用柯里化将多个参数的函数表示为返回函数的方式会导致额外的函数调用开销14 如果没有编译器的优化，那么递归调用可能会十分的占用资源16，部分的编译器对于尾递归的调用方式都用称为TCO（Tail Call Optimization)的尾调用优化，通过这种优化方式，可以极大程度的减少递归调用中的堆栈内存占用。但同时也会破坏原来的程序方法的调用链，引入新的复杂性。因此像Golang这样的语言中，尽管社区对于尾调用的优化有着很高的呼声，但设计者对编译器仍没有增加尾调用的优化，因此在使用函数式思维进行程序设计时，需要考虑的递归调用的堆栈占用问题。 究其本质而言，函数式的语言倾向于比命令性语言生成大量的短期垃圾。 尽管他们的垃圾收集器倾向于针对此使用情况进行优化，但仍然使预测和控制性能更加困难17。 其次是应用场景方面：\n在一些特殊函数方法中往往不能使用函数式的思维，如IO方法并不能纯函数化，因为IO依赖于一些外部的状态和side-effect，不符合纯函数的要求16。 在某些应用场景中计算机无法按函数式的方式运行，如一些有状态和命令式的操作，他们本质上就是一个巨大的状态机，FP在这里无法起作用16。 同理，其他一些有状态的对象，在OOP中可以很好的处理应对，但是对于FP来说，就是十分棘手的困难。 总结 在本文中，我简单介绍了函数式语言的发展，并概述了函数式的设计思维在一些已有场景的应用，同时从正反两个角度分别阐述了函数式程序设计的优越性和局限性。\n综上所述，函数式程序设计经过了很长一段时间的发展，人们已经总结出了许多行而有效的应用方法，但它并不是银色子弹，不能一劳永逸地解决我们程序设计中的所有问题，恰恰相反，函数式程序设计的应用往往会给我们带来新的问题。我们应当从函数式中学到一种我们在面向对象的程序设计中接触不到的程序设计理念，并将其很好的与OOP进行结合。\n事实也证明FP与OOP是正交的，它们可以很好的结合起来，针对不同问题各取所长。OOP强调对象状态的改变，混合了声明与状态的概念，而FP有着天生适合并发的优势，以及自解释等提高代码质量的优点。\n细细品味函数式编程，就会觉得FP天生就有一种独特的魅力，我们可以好好利用函数式编程的思维来实现某些符合函数式特性场景的具体需求。所以，在今后的学习工作中，也应当做到具体问题具体分析，运用好函数式程序设计这个巧妙的工具来解决我们遇到的各种问题，提高自身的程序设计能力和代码软件的质量。\n参考文献 佚名. 函数式编程[Z/OL](2020–12–21).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佚名. Haskell[Z/OL](2020–12–19).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGEISON. Functional Go[EB/OL](2020–04–05).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDEEPU. 7 Easy Functional Programming Techniques in Go[EB/OL](2019–08– 14).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佚名. 静态单赋值形式[Z/OL](2020–09–21).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAPPEL A W. SSA Is Functional Programming[J/OL]. ACM SIGPLAN Notices, 1998, 33(4): 17–20. DOI:10.1145/278283.278285.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPEYTON JONES S L. Parallel Implementations of Functional Programming Languages[J/OL]. The Computer Journal, 1989, 32(2): 175–186. DOI:10.1093/comjnl/32.2.175.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHARGREAVES M. The Case for Functional Programming and Serverless Architecture[EB/OL](2018–06–06).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGAWROŃSKI W. Functional Programming in Serverless World | Pattern Match[EB/OL](2018–10–18).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFRITSCH J. Functional programming languages in computing clouds : practical and theoretical explorations[C].\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMOCCHETTI R. Using Functional Programming When Building Cloud Native Applications with AWS Lambda[EB/OL]([日期不详]).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSOSHNIKOV D. Using Functional Programming for Development of Distributed, Cloud and Web Applications in F#[J]. 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPIKE R. command center: Self-referential functions and the design of options[EB/OL](2014–01–24).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佚名. Functional Programming Is Not What You (Probably) Think - DZone Java[EB/OL]([日期不详]).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佚名. Find Out Pros and Cons of Functional Programming[EB/OL](2018–06– 07).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佚名. 5 Drawbacks of Functional Programming – Plus 4 Reasons to Use It[EB/OL](2019–08–29).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佚名. Tikhon Jelvis’s answer to What are some limitations/disadvantages of functional programming? Where does it break down when you want to get things done? - Quora[EB/OL](2014).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2021-03-27T00:00:00Z","image":"https://blog.kagaya.fun/p/2021/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%BD%B1%E5%93%8D%E4%B8%8E%E5%BA%94%E7%94%A8/87461089_p1_hu_9ee647509f0c9614.png","permalink":"https://blog.kagaya.fun/p/2021/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%BD%B1%E5%93%8D%E4%B8%8E%E5%BA%94%E7%94%A8/","title":"函数式程序设计的影响与应用"},{"content":"使用Kubernetes V1.20.0 与 Containerd 配置K8s集群 写在前面 Kubernetes在最新的1.20.0版本中表示正式放弃了Docker，原来的Docker项目也早已改名为Moby了，即便Docker在容器技术中仍占据主流，并且在其他地方也能够继续发光发热，但从长远来看，容器运行时必然也会进入一个多方竞争的时代。对于适配了Kubernetes CRI的容器运行时，我们便有了很多选择，如Containerd和CRI-O。Docker目前占据了约七成的份额，而第二名Containerd也占据了两成多。刚好实验室有搭建集群的需求，作为爱折腾星人，决定紧跟时代，用最新的V1.20.0的Kubernetes与Containerd来搭建我们的实验集群。我会简单讲解我的配置过程以及中间遇到的问题及解决方法，毕竟自己踩过的坑，得讲一遍才能记得深刻（笑）\n本文所需shell脚本都在这个repo，如果之后有需要也会不断更新，对于配置过程中产生的问题欢迎在issue中讨论。\n配置基本环境 这次打算在四台裸机上部署一个单master节点，三个worker节点的小集群，单机配置如下：\n当然这是我部署好后的截图，主机的名称也改为了ip+nodeType，方便识别当前主机，当然这也是必须修改的，因为原来的四台主机的主机名都为localhost.localdomain，同名主机在建立集群后不能一起显示，这也是配置时遇到的坑之一，所以我也同样将改名的命令分别写在了04_pull_master_images.sh与04_pull_worker_images.sh，可以自动实现将主机名替换为ip+nodeType，当然，这都是后话了。\n本次主要是在CentOS7下进行配置，要求能连接外网，对于kubernets需要的k8s.gcr.io上的镜像我也已经在阿里云上同步了一份，所以没有挂代理的要求。当然最好还是在root用户下执行，不保证非root用户的执行效果（可增加sudo尝试）\n对于每一台机器，只需要一次执行00～02对应脚本\n1 2 3 4 # cd进入脚本对应目录 bash 00_install_tools.sh bash 01_config_env.sh bash 02_install_kube.sh 00安装了一些必要的工具，如git、vim、unzip、ntpdate等，安装了yum epel并对yum进行了换源。除此之外，还包含一些常用的工具如zsh、neofetch、htop等，同时也会将shell切换为zsh（⚠️注意，后文的脚本都是基于修改~/.zshrc文件，如果不使用zsh，请对应修改为~/.bashrc或~./bash_profile），这里可以根据自己的需要做修改。\n01设置了一些基本环境配置，关闭firewalld和swap，设置了iptable转发等，特别是对时间进行同步，并修改为CST时区，保证各个主机上时间的一致性对集群的配置也至关重要。\n02为yum添加了Kubernetes的repo，安装了1.20.0版本的kubelet、kubeadm、kubectl三件套\n安装Containerd 接下来，我们为每一台主机安装Containerd替换之前的Docker。\n一键安装：\n1 bash 03_install_containerd.sh Containerd的官方quick-start文档会默认你已经安装了runc（Containerd默认的容器运行工具），所以第一步我们还是要安装runc，否则将无法运行容器。\n我们直接选择拉取runc的官方仓库进行编译安装，runc比较小，编译很快，但是需要有go的运行环境和libseccomp 、libseccomp-devel两个链接库，可以参考这里。这些都在脚本中安装好了，如果你不想编译，也可以下载编译好的二进制文件放到$PATH对应的目录中即可。\n1 2 3 4 5 # install runc git clone https://github.com/opencontainers/runc cd runc \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 这样就安装好了runc工具\n安装Containerd也很简单，直接下载官方编译好的二进制压缩包，解压对应目录，这里我也选择了较新的1.4.3版本，甚至官方仓库的release tag里还是1.3.9版本\n1 2 3 CONTAINERD_VERSION=1.4.3 wget https://github.com/containerd/containerd/releases/download/v\u0026#34;$CONTAINERD_VERSION\u0026#34;/containerd-\u0026#34;$CONTAINERD_VERSION\u0026#34;-linux-amd64.tar.gz tar -xvf containerd-\u0026#34;$CONTAINERD_VERSION\u0026#34;-linux-amd64.tar.gz -C /usr/local/ 接下来就是比较关键也是我当时遇到问题最多的地方：containerd配置\n一般来说，containerd提供了生成默认配置的方法\n1 2 3 # 需提前创建对应目录 # sudo mkdir -p /etc/containerd/ containerd config default \u0026gt;/etc/containerd/config.toml 但是这样会有一些问题：\n之后在使用crictl工具连接containerd工具时会报错：\n1 unknown service runtime.v1alpha2.ImageService \u0026amp;Unimplemented desc = unknown service api.v1.CRIPluginService 可参考这个issue，对应的解决方法就是修改以下配置\n1 2 3 4 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd] snapshotter = \u0026#34;overlayfs\u0026#34; --\u0026gt; \u0026#34;native\u0026#34; default_runtime_name = \u0026#34;runc\u0026#34; no_pivot = false 后面在使用kubeadm init初始化集群时，在启动apiserver等pod时，会报错：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Unfortunately, an error has occurred: timed out waiting for the condition This error is likely caused by: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands: - \u0026#39;systemctl status kubelet\u0026#39; - \u0026#39;journalctl -xeu kubelet\u0026#39; Additionally, a control plane component may have crashed or exited when started by the container runtime. To troubleshoot, list all containers using your preferred container runtimes CLI. Here is one example how you may list all Kubernetes containers running in cri-o/containerd using crictl: - \u0026#39;crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | grep kube | grep -v pause\u0026#39; Once you have found the failing container, you can inspect its logs with: - \u0026#39;crictl --runtime-endpoint /run/containerd/containerd.sock logs CONTAINERID\u0026#39; couldn\u0026#39;t initialize a Kubernetes cluster 使用systemctl status kubelet查看kubelet情况，可以看到kubelet有在运行但是日志里一直打印node \u0026quot;localhost.localdomain\u0026quot; not found\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Thu 2020-12-17 20:48:18 CST; 6min ago Docs: https://kubernetes.io/docs/ Main PID: 10688 (kubelet) Tasks: 18 Memory: 37.6M CGroup: /system.slice/kubelet.service └─10688 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf ... Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.161585 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.261944 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.362297 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.462629 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.562916 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.663161 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.763546 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.863925 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:35 localhost.localdomain kubelet[10688]: E1217 07:54:35.964162 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found Dec 17 20:54:36 localhost.localdomain kubelet[10688]: E1217 07:54:36.064368 10688 kubelet.go:2240] node \u0026#34;localhost.localdomain\u0026#34; not found 这里的日志不够详细，于是我又使用journalctl -xeu kubelet查看更详细的日志，日志数量比较多，很容易漏掉关键的错误，比如说下面这条报错是我在重试了多次后才在日志中发现的：\n1 CreatePodSandbox for pod \u0026#34;kube-scheduler-localhost.localdomain_kube-system(a28385ea64639c19ce25476016scheduler-localhost.localdomain_kube-system(a28385ea64639c19ce254760161b1d3b)\u0026#34; failed: rpc error: code = Unknown desc = failed to get sandboxb1d3b)\u0026#34; failed: rpc error: code = Unknown desc = failed to get sandbox image \u0026#34;k8s.gcr.io/pause:3.1\u0026#34;: failed to pull image \u0026#34;k8s.gcr.io/pause:3.1\u0026#34;: failed to pull image \u0026#34;k8s.gcr.io/pause:3.1\u0026#34;: failed to pull and unpack image \u0026#34;k8s.gcr.io/pause:3.1\u0026#34;: 可以看到中的关键信息failed to pull image \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;，也就是说kubelet创建pod时卡在了拉取pause镜像这一步，当时在初始化kubeadm之前，我已经在本地准备好了所有所需要的镜像，并且kubeadm通过指定镜像仓库的参数--image-repository=${IMAGE_REPOSITORY}已经将kubelet所要拉取镜像的目标仓库切换为阿里云的仓库，但这里还是去从一个不存在的谷歌容器仓库k8s.gcr.io拉取镜像，导致我在这里一度怀疑是kubelet的配置问题。\n直到我翻到了kubeadm的issue#2020中提到了kubeadm并不会配置pause image的repository，以及在containerd的issue#813，提到了pause镜像的仓库及版本是写在containerd的配置文件中，所以，只需要在config.toml中修改以下配置\n1 2 3 4 5 6 7 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] disable_tcp_service = true stream_server_address = \u0026#34;127.0.0.1\u0026#34; stream_server_port = \u0026#34;0\u0026#34; stream_idle_timeout = \u0026#34;4h0m0s\u0026#34; enable_selinux = false sandbox_image = \u0026#34;k8s.gcr.io/pause:3.1\u0026#34; --\u0026gt; \u0026#34;registry.cn-hangzhou.aliyuncs.com/kagaya/pause:3.2\u0026#34; 以上问题就解决了:-(\n所以，配置脚本中直接生成了修改好的配置文件，使用脚本就不会遇到以上问题了\n安装Kubernetes并创建集群 以上的操作是需要在每一台主机进行配置，接下来的操作可能会有Master与Worker主机的区别，请注意。\n这里先使用上一步配置好的crictl工具拉取镜像为创建集群时节省时间\n准备Master节点镜像\n执行以下脚本\n1 bash 04_pull_master_images.sh 准备Worker节点镜像\n1 bash 04_pull_worker_images.sh 以上两步会以ip+nodeType的格式同时修改Master和Worker节点的主机名，原因写在了文章开头\n初始化Master节点\n1 bash 05_init_master.sh 这一步会在kubeadm中指定kubernetes的版本，CIDR范围，镜像仓库等，如果顺利的话，你可以看到最后打印的其他节点join集群的方法，类似这样：\n1 2 kubeadm join 192.168.0.170:6443 --token r7w69v.3e1nweyk81h5zj6y \\ --discovery-token-ca-cert-hash sha256:1234a2317d27f0a4c6bcf5f284416a2fb3e8f3bd61aa88bc279a4f6ef18e09a1 之后需要手动在各个Worker节点上执行一下这条命令加入Master所在集群，请务必确保各节点时间同步，因为token的合法性会用时间去验证。\n如果需要，你还可以使用05_init_worker_kubectl.sh来初始化Worker节点上的kubectl命令\n如果以上步骤顺利的话，你现在可以在Master节点上使用kubectl get nodes -o wide看到各个节点的信息，当然Status为NotReady是因为我们还没有执行最后一步，安装calico网络\n安装Calico网络 这一步就比较简单了，只需要在每个Node上都从Dockerhub上拉取对应的镜像，然后从Master节点上用kubectl应用yaml文件。\n一键脚本：\n1 bash 06_install_calico.sh 这里也选用了较新的Calico v3.17.1\n1 2 3 4 5 6 CALICO_VERSION=v3.17.1 crictl pull calico/cni:$CALICO_VERSION crictl pull calico/pod2daemon-flexvol:$CALICO_VERSION crictl pull calico/node:$CALICO_VERSION crictl pull calico/kube-controllers:$CALICO_VERSION 默认的yaml文件来自官方的3.17版本，我也在manifests目录下准备了离线版本\n1 2 CALICO_YAML=\u0026#34;https://docs.projectcalico.org/v3.17/manifests/calico.yaml\u0026#34; # CALICO_YAML=\u0026#34;./manifests/calico.yaml\u0026#34; kubernetes会在各个节点上使用DaemonSet的方式运行calico pod，这样各个节点的pod就可以通过虚拟网络进行通信了。\n最后说几句 以上就基本完成了K8s集群的搭建，如果需要，还可以安装metrics server以提供各个节点的Cpu与Mem资源信息\n1 bash 07_install_metrics_server.sh 当然，文中所提到的问题并不是全部，其他诸如安装metric server时的错误：\n1 metric-server : TLS handshake error from 20.99.219.64:57467: EOF 需要通过修改metric server deployment的启动参数解决\n1 2 3 - args: - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname 如果有其他问题，也欢迎在这里提交issue交流。\n最后：\nKill Docker And Long Live Kubernetes!!!\n","date":"2020-12-17T23:31:25+08:00","image":"https://blog.kagaya.fun/p/2020/%E4%BD%BF%E7%94%A8kubernetes-v1.20.0-%E4%B8%8E-containerd-%E9%85%8D%E7%BD%AEk8s%E9%9B%86%E7%BE%A4/k8s_hu_89713466cd9e49e0.png","permalink":"https://blog.kagaya.fun/p/2020/%E4%BD%BF%E7%94%A8kubernetes-v1.20.0-%E4%B8%8E-containerd-%E9%85%8D%E7%BD%AEk8s%E9%9B%86%E7%BE%A4/","title":"使用Kubernetes V1.20.0 与 Containerd 配置K8s集群"},{"content":"使用Docker部署本地Wordpress开发环境 为了便于维护开发线上的wordpress站点，在本地构建一个wordpress开发环境。使用Docker来实现轻量化开发与部署环境的构建。\n背景 由于最近接手了一个网站的运维工作，使用的还是Wordpress，因为有涉及到一些布局的调整，因此决定在本地构建一个开发调试环境用于进行上线前的测试，但又不想离开本机中Vscode等开发环境（如代码提示等）。在考虑了包括虚拟机LANMP，本机直接构建环境等方案后，最后选择了Docker来部署开发环境，同时也可以进一步熟悉Docker的使用方法。有了Docker，免去了虚拟机中繁杂的环境配置过程，同时对环境进行模块化解耦后可以十分便捷的启动或更新开发环境组件，也使得开发环境尽可能的贴近生产环境，之后的部署上线将变得十分容易。\n本机环境 MacOS 10.15.6（理论上适用于所有支持Docker的环境）\nDocker Desktop 19.03.12\n镜像准备 1 2 3 4 $ docker images nginx latest 7e4d58f0e5f3 12 days ago 133MB mysql 5.7.31 ef08065b0a30 13 days ago 448MB daocloud.io/library/php 7.2.24-fpm 48432d192e1a 11 months ago 398MB 由于使用了Wordpress，所以环境的搭建涉及到了三个部分，ngin、mysql以及php，为了与生产环境一致，特别选择了对应的mysql与php的版本。理论上也不建议选择latest版本的镜像，选择相对稳定的版本会有更好的兼容性。\n可以从docker hub上搜索对应版本的镜像\n拉取镜像 1 2 3 $ docker pull nginx:latest $ docker pull mysql:5.7.31 $ docker pull daocloud.io/library/php:7.2.24-fpm 构建Nginx+PHP环境 Nginx配置 Nginx的镜像是不能解析php文件的，因此我们需要为nginx配置php-fpm来负责解析处理php代码并返回处理结果。\n为此，我们需要做一下几件事情：\n修改nginx的配置，指定php处理方式 通过设置使nginx能够访问到php容器 启动nginx与php的容器 首先，为了持久化配置，我在/Users/kagaya/Docker/nginx/conf.d/下新建了default.conf配置文件，挂载到目标文件/etc/nginx/conf.d:ro并指定只读属性\n最后对nginx的配置文件做对应修改即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # default.conf server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.php index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location ~ \\.php$ { fastcgi_pass php:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /var/www/html/$fastcgi_script_name; include fastcgi_params; } } 同样，为了方便开发，可以把nginx中的网页目录映射到主机中，这里使用了Users/kagaya/docker/www作为主机中的网站根目录，使用-v将其挂载到nginx中的/usr/share/nginx/html\n使用Docker 挂载主机配置目录到容器，并使用--link参数来连接php容器，docker link采用了修改Hosts文件的方式使得可以通过自定义的名称访问到对应的ip地址，使得容器之间能通过网络互相访问\n最后使用-p 80:80将nginx的80端口开放到主机的80端口，使得主机可以通过localhost访问到nginx\nPHP配置 php配置相对简单，只需要挂载网站根目录到容器即可\n1 $ docker run -d --name php -v /Users/kagaya/docker/www:/var/www/html daocloud.io/library/php:7.2.24-fpm 使用Docker-compose一键部署多个服务 由于涉及到了多个容器服务，可以采用Docker-compose工具来方便的同时配置部署多个容器服务\n新建目录，如webdev，目录下新建docker-compose.ym文件，配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 version: \u0026#39;3\u0026#39; services: nginx: image: nginx:latest container_name: nginx ports: - \u0026#34;80:80\u0026#34; depends_on: - php - mysql volumes: - \u0026#34;/Users/kagaya/docker/nginx/conf.d:/etc/nginx/conf.d:ro\u0026#34; - \u0026#34;/Users/kagaya/docker/www:/usr/share/nginx/html\u0026#34; links: - \u0026#34;php:php\u0026#34; php: image: daocloud.io/library/php:7.2.24-fpm container_name: php volumes: - \u0026#34;/Users/kagaya/docker/www:/var/www/html\u0026#34; 在www目录下放一个index.php文件输出phpinfo用于测试，最后在浏览器里访问localhost\n在Docker内连接Mysql 对于Wordpress来说，需要配置对应的mysql数据库，为此，我们需要让nginx和php能够访问到mysql容器，于是我们同样可以使用Link方式来实现\n同时，我们还可以将mysql的3306端口映射到主机，方便我们用数据库管理软件进行操作\n如果相对数据库中的数据进行持久化，也可以把mysql容器中的/var/lib/mysql目录映射出来\n1 $ docker run -d --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7.31 在配置好以上服务后，我们可以对wordpress目录下的wp-config.php文件进行配置，如数据库连接，注意：数据库的Host此时为link后的名称，如在运行nginx和php时使用了如--link myslq:db的参数，那么在容器中对于的数据库主机则为db\n打开Wordpress调试模式，这样可以及时在浏览器中看到错误信息：\n1 2 3 4 5 6 7 8 9 10 11 /** * 开发者专用：WordPress调试模式。 * * 将这个值改为true，WordPress将显示所有用于开发的提示。 * 强烈建议插件开发者在开发环境中启用WP_DEBUG。 * * 要获取其他能用于调试的信息，请访问Codex。 * * @link https://codex.wordpress.org/Debugging_in_WordPress */ define(\u0026#39;WP_DEBUG\u0026#39;, true); 本次配置中使用的PHP版本为7.2.24，在PHP7以后，mysql_connect函数被完全移除了，而wordpress为了兼容会先检测php环境中有没有mysqli模块，否则会使用原来的mysql_connect函数。\n需要注意的是PHP的镜像中为了最小化镜像体积，是不包含mysqli模块的，当然，官方也很方便的提供了docker-php-ext-install工具来方便我们安装模块，因此需要我们使用Dockerfile文件安装mysqli模块后再次封装新的镜像。\n在webdev下新建Dockerfile文件：\n1 2 FROM daocloud.io/library/php:7.2.24-fpm RUN docker-php-ext-install mysqli 最终的docke-compose.yaml文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 version: \u0026#39;3\u0026#39; services: nginx: image: nginx:latest container_name: nginx ports: - \u0026#34;80:80\u0026#34; depends_on: - php - mysql volumes: - \u0026#34;/Users/kagaya/docker/nginx/conf.d:/etc/nginx/conf.d:ro\u0026#34; - \u0026#34;/Users/kagaya/docker/www:/usr/share/nginx/html\u0026#34; links: - \u0026#34;php:php\u0026#34; - \u0026#34;mysql:db\u0026#34; php: # image: daocloud.io/library/php:7.2.24-fpm build: . container_name: php volumes: - \u0026#34;/Users/kagaya/docker/www:/var/www/html\u0026#34; links: - \u0026#34;mysql:db\u0026#34; mysql: image: mysql:5.7.31 container_name: mysql ports: - \u0026#34;3306:3306\u0026#34; environment: MYSQL_ROOT_PASSWORD: \u0026#34;123456\u0026#34; 最后，只需要在docker-compose.yaml所在目录运行以下命令\n1 $ docker-compose up -d 初次运行会自动编译镜像，之后每次运行配置不发生改变就会启用原来的容器，若配置发生改变，也会自动重建对应容器\n这样，我们就可以直接在本地调试wordpress中的php代码了，并且在vscode中也可以获得代码提示\n","date":"2020-09-23T16:47:28+08:00","image":"https://blog.kagaya.fun/p/2020/%E4%BD%BF%E7%94%A8docker%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0wordpress%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/CleanShot_hu_359d14381b97fd13.png","permalink":"https://blog.kagaya.fun/p/2020/%E4%BD%BF%E7%94%A8docker%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0wordpress%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/","title":"使用Docker部署本地Wordpress开发环境"},{"content":"八佰看完了随便谈谈 我先给《八佰》一个好评 《八佰》看完了，还专门去看了IMAX版。尽管这部电影从宣发到撤档再到重新定档上映，之间备受争议。但我还说要说，仅从抗战电影本身来说，这是部好电影，甚至超出了我之前看过的所有抗战片，我要给它很高的评价。《八佰》的出现对于中国抗战片来说有一定的特殊意义，类似《流浪地球》对于中国科幻片。这不仅因为它是一部少有的大陆拍摄的国军战役，而且制作团队对于电影的取材，伴奏配乐，画面分镜以及叙事节奏的掌握都十分出色。最后所表达的主题也很明确：**尽管这是一场必然失败的“表演”战役，但是“八佰壮士”用自己的行动向江对岸的人展示了中国军人真正的气魄，以此来唤醒更多中国人参与抗战。**这点我深表赞同。\n个人认为唯一的争议 说说争议点，对我来说唯一让我非常不满的是导演管虎本人，《八佰》是部好电影，但是宣发的时候特意要去找八十八师师长“飞将军”孙元良的后人，孙的行为在抗战期间颇有争议，尽管是孙元良当时亲自给谢晋元下令死守四行仓库，但是因为本人的污点，不应该成为宣发时的素材。\n历史虚无主义？ 历史虚无主义——指不加具体分析而盲目否定人类社会的历史发展过程\n有人批评电影有历史虚无主义的嫌疑，认为一方面历史中日军并没有猛攻四行仓库，且电影中刻意夸大了国军的表现，认为事实中的国军并不是为了保卫上海而在电影中被“美化”，女学生送国旗也是从桥上光明正大地走过去而不是冒着子弹渡河。确实，电影确实在这方面有所夸张，但是电影之所以是电影而不是纪录片，为了表现主题而做适当的夸张处理，以此使感情的表达更加强烈，我觉得是可以接受的，在这一点上《八佰》确实也做到了。关于“美化”的问题，我觉得电影中处理的也比较恰当，如果我们交换时空设想一下，当时河对岸租界里的普通民众看来，对岸的中国军人在对抗日寇就是在保护自己（尽管他们也知道日军不会打到租界来），从谢晋元以及仓库里的士兵这些以执行军令为天职的军人看来，上级下达死守仓库的命令就是为了保卫上海最后一块领地而他们或许真的不知道这背后的真实目的是什么，电影中也表现了谢晋元团长的军人风度和民族气概以及士兵们视仓库为自己最后的坟墓，绝不后撤保卫上海的决心。在电影的最后，特使和谢团长的对峙中，戳破了这场战斗只是“表演”，只是为了在国际上获得同情，也是基本尊重史实的。所以我认为《八佰》出色地完成了它的任务，可以说它有虚构的成分，但最后宣扬的主题是没有问题的。既没有否定国军正面战场的妥协失利利用“表演”来博得国际社会同情的行为，也没有否定国军中也有像谢晋元及其部下英勇抗战的中国军人。\n电影是好电影 电影最出彩的看点我认为是河两岸的这种天堂与地狱强烈对比，也是四行仓库保卫战带上一股特殊悲壮气氛的关键点，而电影中也得到了很着重的描写刻画，两岸如此相近又如此遥远，这种矛盾冲突让整部电影都笼罩在一种特殊的氛围当中，而且越是比较，越是压抑，一次次地用镜头画面冲击人心。电影中也运用了不少第一人称的镜头，无论是冲锋时突然满眼的鲜红色，还是深夜时对岸的灯红酒绿，都带给观众很强的心灵震撼。电影中也巧妙使用了关公意象，从对岸戏台上的表演到小湖北的想象，让观众更能体会到这场战役中仓库守军所面临的困境以及为国赴死的决心。\n结局最后在“八百壮士”从桥面冲向租界撤退戛然而止，并在片尾拍摄了如今上海四行仓库的遗址，串起了历史与当今，这种对比让人更加感同身受。这是很标准的结尾，不出彩，但值得好评。\n总结 总而言之，制作团队的努力不可否认，淞沪会战“八百壮士”的英雄事迹不该遗忘，有机会一定也去四行仓库遗址看看。最后，我希望这部电影能够走出中国，毕竟长期以来，欧洲战场太平洋战场的好片子太多了，作为东方主战场的中国，需要更多类似《八佰》这样的电影，在尊重历史的基础上，向世界展示中国在世界反法西斯战争中的决心和努力，无论是共军还是国军。\n再给管大导演一个机会，期待他后面抗美援朝的片子。。。\nps: 金刚川看完了，属实给人看嗯了。。。\n","date":"2020-08-17T22:42:52+08:00","image":"https://blog.kagaya.fun/p/2020/%E5%85%AB%E4%BD%B0%E7%9C%8B%E5%AE%8C%E4%BA%86%E9%9A%8F%E4%BE%BF%E8%B0%88%E8%B0%88/%E5%85%AB%E4%BD%B0_hu_4ff3a576161ca6a9.jpg","permalink":"https://blog.kagaya.fun/p/2020/%E5%85%AB%E4%BD%B0%E7%9C%8B%E5%AE%8C%E4%BA%86%E9%9A%8F%E4%BE%BF%E8%B0%88%E8%B0%88/","title":"八佰看完了随便谈谈"},{"content":"DeepClone深克隆 之前去参加字节跳动面试的时候遇到了很尴尬的情况，连续两个面试官让我手写一个深克隆出来，尽管我明白需要深克隆的原因是什么，然而之前准备的时候虽然看到了有关深克隆的相关概念，因为看到“需要付出一定的性能代价，因此平常使用中需要尽量避免”这句话，于是便没有深究实现方法，结果当然也是没能够写出来。\n于是打算利用这篇blog彻底理解这方面的概念和实现方法，也加深对JavaScript这门语言的认识\n为什么会有深克隆的问题——谈谈弱类型 有深克隆就会有浅克隆，出现这两者的原因还是由于JavaScript的类型机制\n说到类型机制，JavaScript作为一门弱类型的语言，在变量类型上帮助程序员做了太多的工作，由于我之前是以写C/C++为主，在刚接触JavaScript、Python和PHP的时候，惊奇于竟然有语言不需要再申明变量的时候使用类型定义符，感叹于总算不需要为某种变量不知该定义哪种类型头疼了。\n于此同时，对于Python，PHP这种新变量直接赋值便可以使用的来说，我更喜欢JS的原因是有用于变量声明的var，const，let，因为这更贴近我所熟悉的C++语法，而PHP使用变量都需要打一个$符号也让我十分不习惯\n当然有利就有弊，弱类型语言带来的最大的问题就是变量类型的不明确以及对于编辑器无法做出类型推导等。弱类型语言使得编译器（或解释器）不太去关注变量的类型，同时也会帮程序员做一些看不见的类型转换，虽然一定程度上让人可以更加专注于程序逻辑上，但往往会埋下一些潜在的问题\n用一个最简单的例子\n1 2 3 4 //javascript var a = 1 a = \u0026#34;123\u0026#34; var b = a + 1\t// \u0026#39;1231\u0026#39; 1 2 3 4 # python a = 1 a = \u0026#39;123\u0026#39; b = a + 1 # type error JavaScript是直接可以字符串+int值的，然而python中会报错，但是两者对于一个int型的变量赋值一个字符串都是合法的。这在c++里是不可想象的。\n这就导致了在函数传参的时候又可能传进去参数的类型并不是所期待的类型，而这种错往往很难发现。\n好在对于JavaScript，微软发明了一种TypeScript的语言作为JavaScript的超集，很好的解决了JavaScript最大的痛点，类型问题，而且在编辑器里有着很好的类型推导，这简直让人更加喜爱这门语言\n话说回到深克隆问题\n用过C++的人都知道，C++最大的一个优势之一就是凭借着指针类型变量可以方便的直接对内存进行操作。而对于JavaScript其实也是有指针变量，虽然并没有单独划分出一个类型而且功能没C++那么强大，但都是表示指向了内存的一块区域。\n当时在学习C++类的拷贝构造函数时，老师就特意提醒对于类中的指针变量不可直接赋值，需要申请新的内存空间然后使用memcpy等函数进行内存复制。这是因为直接进行指针的赋值会导致多个指针指向同一块内存，引起访问冲突。而这，正是导致JavaScript中出现浅拷贝深拷贝问题的根源。\n1 2 var a = [1, 2, 3] var b = a 在这里，a和b可以看作两个指针，同时指向了一个数组，对其中一个做出修改，另一个也会同时变化。\n如果要实现一个浅克隆可以这样做\n1 2 3 4 5 6 7 8 9 10 11 var a = [1, 2, 3] var b = [] for(let i = 0; i \u0026lt; a.length; i++) { b.push(a[i]) } // 或者借助Object的assign方法 b = Object.assign([], a) // ES5写法 b = a.concat() // 使用ES6的扩展运算符 b = [...a] ⚠之所以称之为浅克隆是因为对于有多层嵌套的对象，里面可能有不同的数据类型，使用浅克隆并不能赋值深层对象的内存空间，仍然会出现多个变量指向同一个内存空间的情况，这种情况下，我们就需要深克隆\n在js中有一个小技巧可以实现一个有限的深克隆\n1 2 var a = {a: 123, b: {c: \u0026#39;356\u0026#39;, d: [7, 8, 9]}} var b = JSON.parse(JSON.stringify(b)) 利用JSON转为字符串在进行json解析，可以实现一个方便的深克隆，但是要求对象中只能是一些数值类型变量，对于对象中的函数，以及嵌套引用却无能为力\n接下来从JavaScript的数据类型开始讨论深克隆的实现\nJavaScript数据类型 JavaScript总共有七种数据类型\n六种原始数据类型 Undefined, Null, Boolean, Number, String, Symbol 一种引用类型 Object\n对于原始数据类型，是直接可以通过等号=进行复制的，相当于C++中的普通类型变量\n而对于Object类型变量，JavaScript是通过变量引用一块内存空间实现的，相当于C++的指针类型变量\n因此，我们需要能够区分变量类型从而做出不同的处理\n使用typeof运算符只能区分以上七种以及’function‘\n对于Object中类型的区分，需要借助Object原型链上的toString函数，可以将数据类型表示为类似\u0026rsquo;[object Array]\u0026lsquo;的字符串，可以很好的区分基本数据类型以及各种object\n特殊的，对于DOM元素需要使用instanceof来判断，若直接用toString判断，不同标签会返回不同的字符串\n这样，实现一个准确判断变量类型的函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 function type(obj) { var toString = Object.prototype.toString; var map = { \u0026#39;[object Boolean]\u0026#39; : \u0026#39;boolean\u0026#39;, \u0026#39;[object Number]\u0026#39; : \u0026#39;number\u0026#39;, \u0026#39;[object String]\u0026#39; : \u0026#39;string\u0026#39;, \u0026#39;[object Function]\u0026#39; : \u0026#39;function\u0026#39;, \u0026#39;[object Array]\u0026#39; : \u0026#39;array\u0026#39;, \u0026#39;[object Date]\u0026#39; : \u0026#39;date\u0026#39;, \u0026#39;[object RegExp]\u0026#39; : \u0026#39;regExp\u0026#39;, \u0026#39;[object Undefined]\u0026#39;: \u0026#39;undefined\u0026#39;, \u0026#39;[object Null]\u0026#39; : \u0026#39;null\u0026#39;, \u0026#39;[object Object]\u0026#39; : \u0026#39;object\u0026#39; }; if(obj instanceof Element) { return \u0026#39;element\u0026#39;; } return map[toString.call(obj)]; } 实现DeepClone 简单版——未考虑循环引用以及dom元素\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 function deepClone(data) { var t = type(data), o; if(t === \u0026#39;array\u0026#39;) { o = []; }else if( t === \u0026#39;object\u0026#39;) { o = {}; }else { // 直接赋值的变量 return data; } if(t === \u0026#39;array\u0026#39;) { for (let i = 0; i \u0026lt; data.length; i++) { o.push(deepClone(data[i])); } return o; }else if( t === \u0026#39;object\u0026#39;) { for(let i in data) { o[i] = deepClone(data[i]); } return o; } } 对于function，直接引用复制即可，因为函数中的this是在运行是才决定指向，对于新的对象，调用的函数即使是原对象的函数引用，this也能正确指向\n考虑循环引用的版本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 /** * 区分Object数据类型 * @param {Object} obj * @returns {String} typename */ function objType(obj) { if (typeof obj !== \u0026#39;object\u0026#39;) return \u0026#39;notObject\u0026#39; var toString = Object.prototype.toString var map = { \u0026#39;[object Array]\u0026#39;: \u0026#39;array\u0026#39;, \u0026#39;[object RegExp]\u0026#39;: \u0026#39;regExp\u0026#39;, \u0026#39;[object Date]\u0026#39;: \u0026#39;date\u0026#39;, \u0026#39;[object Object]\u0026#39;: \u0026#39;object\u0026#39;, } return map[toString.call(obj)] } /** * 获取正则对象的flag * @param {RegExp} re 正则对象 */ function getRegFlag(re) { var flag = \u0026#39;\u0026#39; if (re.global) flag += \u0026#39;g\u0026#39; if (re.ignoreCase) flag += \u0026#39;i\u0026#39; if (re.multiline) flag += \u0026#39;m\u0026#39; return flag } /** * DeepClone * @param {Object} obj 深克隆对象 * @returns {Object} 克隆后的新对象 */ function deepClone(obj) { // 维护两个数组存储已经克隆过的对象处理循环引用 var sourceList = [] var targetList = [] function _clone(source) { // null 会被typeof认为是object 一个一直未被解决的bug if (source === null) return null if (typeof source !== \u0026#39;object\u0026#39;) // 包括function也直接返回 return source let type = objType(source) let proto, target if (type === \u0026#39;array\u0026#39;) { target = [] } else if(type === \u0026#39;RegExp\u0026#39;) { target = new RegExp(source.source, getRegFlag(source)) } else if(type === \u0026#39;date\u0026#39;) { target = new Data(source.getTime()) } else { // 复制原型链 proto = Object.getPrototypeOf(source) target = Object.create(proto) } // 如果已经克隆过 const index = sourceList.indexOf(source) if(index != -1) { return sourceList[index] } sourceList.push(source) targetList.push(target) // 递归克隆 for (let item in source) { target[item] = _clone(source[item]) } return target } return _clone(obj) } ","date":"2019-07-02T21:30:20+08:00","image":"https://blog.kagaya.fun/p/2019/deepclone%E6%B7%B1%E5%85%8B%E9%9A%86/asuka_hu_bd712c59a2d481a.jpg","permalink":"https://blog.kagaya.fun/p/2019/deepclone%E6%B7%B1%E5%85%8B%E9%9A%86/","title":"DeepClone深克隆"},{"content":"Unix系统分析——锁 总结Unix系统中锁的相关知识\nLinux各种锁的实现 原子操作 原子操作是最小的执行单位，保证在执行完毕前不会被其他任何事物或事件打断，也就是说不能有比它更小的执行单位，命名也借用了物理学中物质稳定存在的最小单位——原子。\n原子操作于硬件架构相关，由汇编语言实现，一般用于实现计数等操作\n信号量 信号量，Semaphore，Linux内核信号量以一个初始值创建，用来表示同时可以有几个任务可以访问由信号量保护的共享资源。初始值为1的信号量就是互斥锁，即在同一时间只能有一个任务可以访问信号量保护的共享资源\n实现原理 当一个任务要访问一个由信号量保护的共享资源时，通过信号量提供的操作将信号量的值做减1操作，若此时值变为负数，表示当前资源不可得，该任务必须挂起进入等待队列直到该信号量可用；若值为非负数，表示成功获得信号量，可以访问共享资源。\n在任务访问完毕后，需要释放获得的信号量，即通过对应操作将信号量的值加1实现，同时判断当前的值正负。若为非正数，表示当前在等待队列中有其他任务正在挂起等待，因此需要主动唤醒等待中的任务。\n读写信号量 读写信号量将信号量按访问目的做了分类——读者和写者。读者对于共享资源只能做读操作，不能修改。而写着可以同时对共享资源进行读写。因为两者对共享变量做的操作不同，因此可以用以下访问规则：\n一个读写信号量可以同时拥有多个读者，即多个读者可以同时访问同一个共享资源\n读者获得读写信号量的前提是：\n当前没有被写者拥有\n没有写者在等待信号量释放\n否则，读者需挂起等待写者释放信号量\n写者获得读写信号量的前提是：\n当前没有被其他读者或写者拥有\n没有其他写者在等待信号量释放\n否则，写者需挂起等待直到没有任何访问\n总而言之，写者是独占访问资源的，具有排他性，同一时间只能有一个写者访问但对于多个读者可以同时访问。同时，为了保证写者不被饿死，只要有写者在等待，读者就不能进入访问，即使当前是其他读者在访问。\n在Linux中，提供了将写者降级为读者的方法\n1 void downgrade_write(struct rw_semaphore *sem); 函数用于把写者降级为读者。写者保持读写信号量期间，任何读者或写者都将无法访问该读写信号量保护的共享资源，对于那些当前条件下不需要写访问的写者，降级为读者将使得等待访问的读者能够立刻访问，从而增加了并发性，提高了效率。因此，读写信号量适用于读多写少的情况\n自旋锁 spinlock，自旋锁与互斥锁最大的区别就是任务在调用自旋锁时不会入睡，若该自旋锁已经被占用，则调用者会一直循环等待锁的释放，也就是保持“自旋”。自旋锁适用于保持锁的时间非常短的情况，因此为了提高效率，选择自旋而不是入睡。\n对比信号量来说，自旋锁可以在进程上下文和中断处理上下文中使用，而信号量只能在进程上下文中使用\n在保持自旋锁的期间，进程是不放弃CPU的，而信号量在保持期间是可以放弃CPU的。对于可剥夺的Linux内核，单CPU的情况下，自旋锁只做了关闭中断和开启中断的操作。\n读写锁 读写锁就是对自旋锁按访问目的划分为读者，写者，提高系统的并发度。\n规则如下：\n如果读写锁当前没有读者，也没有写者，那么写者可以立刻获得读写锁，否则它必须自旋在那里，直到没有任何写者或读者 如果读写锁没有写者，那么读者可以立即获得该读写锁，否则读者必须自旋在那里，直到写者释放该读写锁 RCU RCU（Read-Copy Update），对于被RCU保护的共享数据结构，读者不需要获得任何锁就可以访问它，但写者在访问它时首先拷贝一个副本，然后对副本进行修改，最后使用一个回调（callback）机制在适当的时机把指向原来数据的指针重新指向新的被修改的数据。这个时机就是所有引用该数据的CPU都退出对共享数据的操作\n顺序锁 对读写锁的优化，读者不会被写者阻塞，意味着写者在进行写操作时读者仍然可以读取，读写是可以同时发生的，但写者之间依然互斥。\n既然读写可以并行，为了保证数据的正确性依然有许多限制：\n被保护的资源不能有指针，否则当写者修改指针后，读者读取时可能失效 若读者读取时发生了写操作，读者需要重新读取数据 舒徐所适用于读写同时进行的概率较小的场景，允许读写同时进行，性能和并发度都有提高\n生产者消费者问题 问题概述 生产者消费者问题，即共享固定大小缓冲区的两个进程——生产者进程和消费者进程。生产者的主要作用是生成一定量的数据放到缓冲区中，于此同时消费者也在缓冲区中消耗这些数据，然后重复这个过程。主要需要解决的问题是：\n保证生产者不会在缓冲区满是加入数据 消费者也不会在缓冲区空时消耗数据 为保证以上两点，就必须让生产者在缓冲区满时休眠或干脆就放弃数据，等到下次消费者消耗缓冲区中的数据的时候，生产者才能被唤醒，开始往缓冲区添加数据。同样，也可以让消费者在缓冲区空时进入休眠，等到生产者往缓冲区添加数据之后，再唤醒消费者。\n通常采用进程间通信的方法解决该问题，常用的方法有信号灯法等。必须要注意的是防止出现死锁的情况。出现死锁时，两个线程都会陷入休眠，等待对方唤醒自己。\n该问题也能被推广到多个生产者和消费者。\n基本实现 使用信号量保证对锁的操作是原子的，防止死锁\n在只有一对生产者和消费者的情况下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 semaphore fillCount = 0; // 已生产项目数 semaphore emptyCount = BUFFER_SIZE; // 剩余项目数 procedure producer() { while (true) { item = produceItem(); down(emptyCount); putItemIntoBuffer(item); up(fillCount); } } procedure consumer() { while (true) { down(fillCount); item = removeItemFromBuffer(); up(emptyCount); consumeItem(item); } } 在多个生产者和消费者的情况下，还需要一个互斥锁保证一次只有一个进程被执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 semaphore mutex = 1;\t// 互斥锁 semaphore fillCount = 0; semaphore emptyCount = BUFFER_SIZE; procedure producer() { while (true) { item = produceItem(); down(emptyCount); down(mutex);\t// 保证一次只有一个进程在操作缓冲区 putItemIntoBuffer(item); up(mutex); up(fillCount); } } procedure consumer() { while (true) { down(fillCount); down(mutex); item = removeItemFromBuffer(); up(mutex); up(emptyCount); consumeItem(item); } } CAS锁 基本概念 乐观锁与悲观锁 首先区分一下乐观锁和悲观锁的概念\n乐观锁\n总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的情况，这样可以提高吞吐量\n悲观锁\n总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程，适合多写的情况\n什么是CAS CAS即（Compare And Swap），意为比较并替换 CAS是CPU的一个指令，因此是原子操作，保证并发安全 CAS是非阻塞的、轻量级的乐观锁 实现原理 CAS作为一条CPU指令级别的操作，做的操作为：如果要修改的变量为我期待的值，则将其修改为一个新值，否则这次操作失败。因此，当有多个线程尝试使用CAS更新同一个变量时，只有一个线程能成功更新变量的值，其他都失败，但失败的线程不会入睡，而是再次尝试，直到修改成功。\n每个线程做的事情如下\n1 2 3 4 do{ 备份旧数据； 基于旧数据构造新数据； }while(!CAS( 内存地址，备份的旧数据，新数据)) 可以看到，若CAS操作不成功，则重复循环尝试，成功后跳出循环\n用CAS锁改造有界缓冲区为无锁队列 用CAS实现一个无锁队列的基本操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 typedef struct Node { int data; struct Node *next; } Node; Node *head, *tail;\t// 共享变量指向队列的头尾 bool is_empty() { if (head-\u0026gt;next == NULL) { return true; } return false; } void push_back(void* data) { Node* old; Node* newNode = new Node(); newNode-\u0026gt;data = data; newNode-\u0026gt;next = NULL; do { // 获取旧尾结点指针 old = tail; // 使用CAS判断此次写入是否成功，只有值为NULL才可写入新结点 }while (!cas(\u0026amp;(old-\u0026gt;next), NULL, newNode)); tail = newNode; } int pop_front(int *e) { Node *old; do { if(is_empty()) return 0; // 获取旧的头结点 old = head-\u0026gt;next; // 使用CAS判断当前head指针是否还指向old取到的头节点，是，则从队列中取出该节点，否则失败 } (!cas(\u0026amp;(head-\u0026gt;next), old, old-\u0026gt;next)); *e = old-\u0026gt;data; delete old; return 1; } 之后可将这个无锁队列替换到之前读者写者问题中的有界缓冲区，大大提高系统的并发度\n","date":"2019-06-25T21:30:20+08:00","image":"https://blog.kagaya.fun/p/2019/unix%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90%E9%94%81/81377457_p2_hu_637f0a0404f07e0a.jpg","permalink":"https://blog.kagaya.fun/p/2019/unix%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90%E9%94%81/","title":"Unix系统分析——锁"},{"content":"pthread多线程编程初探 POSIX线程（POSIX threads），简称Pthreads，是线程的POSIX标准。该标准定义了创建和操纵线程的一整套API。\n实验要求 使用pthread实现图像卷积的并行计算\n由于之前使用MPI实现了一遍，相比与MPI，pthread是以共享内存的方式实现并行计算，所有有着系统开销更小，并行损耗更小的优点。但是也限制了pthread只能在一台机器上实现并行，不支持分布式计算。\n基本概念 进程与线程 进程是一个程序运行的实例，一个进程对应系统运行中的一个任务，独占一部分系统资源。\n随着人们对于并行计算的需求越来越多，而每次都开多个进程虽然可以实现并行计算，但带来的新的问题——进程间通信。这就需要涉及操作系统提供的进程通信方法（无名管道、有名管道、信号、消息队列、共享内存、Socket、共享文件），无论哪种方式，对于程序原来说需要单独编写通信相关代码还是过于麻烦。因此，懒惰的程序员们便提出了线程的概念。\n线程是更小的任务实例，对于进程来说，每个进程有着自己独立的数据段（全局变量）、BSS段（未初始化的全局变量和静态变量）、以及堆栈段（局部变量），同一个程序的进程还有着共享的代码段。而对于线程来说，为了解决进程间通信的痛点，进程的数据段、bss段、堆在线程间共享，这样线程之间可以共享进程中的大部分数据，而且一个线程创建和销毁的开销的十分小，可以说很好的解决了fork多进程的问题。\n当然新的机制必然会带来新的问题，比如说共享的数据空间就需要考虑写竞争和一些其他的同步问题，这都需要在实践中一点一点的学习\n在后面Unix系统分析的课程中了解到，其实linux中对于线程的管理也是按照进程的方式管理，只不过线程的地址空间与普通进程有区别，所以说linux中最小的任务实例还是进程\nLiunx的pthread 头文件\n1 #include \u0026lt;pthread.h\u0026gt; 创建线程\n1 2 3 4 5 6 int pthread_create( pthread_t *restrict_tidp, const pthread_attr_t *restrict_attr, void *(*start_rtn)(void), void *restrict_arg ); 需要四个参数，分别对应：线程信息结构体，线程创建参数，线程入口函数，函数参数），四个参数均按地址传入，创建成功返回0，不成功代表创建失败\n看一段代码\n1 2 3 4 5 6 // pthread create if(pthread_create(\u0026amp;thread_id, NULL, thread_task, \u0026amp;paras[i]) \u0026lt; 0) { cerr \u0026lt;\u0026lt; \u0026#34;pthread_create error\u0026#34; \u0026lt;\u0026lt; endl; exit(-1); } thread_task为函数指针，对应线程要执行的函数入口，可以看到线程的执行是以一个函数为入口的，将可并行的部分放在函数中单独让线程执行，使用起来确实更加方便\n回收线程\n1 int pthread_join(pthread_t thread, void **retval); 第一个参数为回收的线程号，会阻塞等待，第二个参数对应线程状态结构体，将回收线程的状态复制到对应地址空间，若不关心线程结束状态，可传入NULL\n编译\n由于pthread.h不是标准库函数，之前需要指定-lpthread，但我发现gcc 7.3.0这样会报错找不到定义，只需要编译时指定-pthread参数即可编译成功\n1 g++ pthread.cpp -o pthread -pthread 遇到的问题及解决方法 Id 有三个东西，pid，thread_id，tid\n对于一个进程来说：\npid就是进程id，和thread_id相等 对于一个进程的许多线程来说：\nthread_id与pid不同，而且不是一个从0开始的数 thread_id在该进程内不相同，可能与其他进程的线程thread_id相同 由于之前提到的，linux对于线程的管理也是按进程的方式，所以线程也有一个pid，称为tid，可以使用系统调用获取。 若需要项其他进程的线程发送信息，就需要使用tid来标识对应线程，而不是pid或者thread_id 传递参数时地址共享问题 由于需要传递多个参数，采用结构体task_para传值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 task_para paras; // pthread create for (int i = 0; i \u0026lt; thread_num; i++) { paras.thread_num = thread_num; paras.my_num = i; paras.start_line = i * BmpHeight / thread_num; paras.end_line = paras[i].start_line + BmpHeight / thread_num - 1; if(pthread_create(\u0026amp;thread_id, NULL, thread_task, \u0026amp;paras) \u0026lt; 0) { cerr \u0026lt;\u0026lt; \u0026#34;pthread_create error\u0026#34; \u0026lt;\u0026lt; endl; exit(-1); } cout \u0026lt;\u0026lt; \u0026#34;Thread \u0026#34; \u0026lt;\u0026lt; paras.my_num \u0026lt;\u0026lt; \u0026#34; start.\u0026#34; \u0026lt;\u0026lt; endl; threads.push_back(thread_id); } 传递参数时传递不同地址的结构体\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 task_para* paras = new task_para[thread_num]; // pthread create for (int i = 0; i \u0026lt; thread_num; i++) { paras[i].thread_num = thread_num; paras[i].my_num = i; paras[i].start_line = i * BmpHeight / thread_num; paras[i].end_line = paras[i].start_line + BmpHeight / thread_num - 1; if(pthread_create(\u0026amp;thread_id, NULL, thread_task, \u0026amp;paras[i]) \u0026lt; 0) { cerr \u0026lt;\u0026lt; \u0026#34;pthread_create error\u0026#34; \u0026lt;\u0026lt; endl; exit(-1); } cout \u0026lt;\u0026lt; \u0026#34;Thread \u0026#34; \u0026lt;\u0026lt; paras[i].my_num \u0026lt;\u0026lt; \u0026#34; start.\u0026#34; \u0026lt;\u0026lt; endl; threads.push_back(thread_id); } 以上就是这次实验中总结的一些知识点，当然thread还有许多用法没有去实践，之后用OpenMP来实现卷积时再打算总结一下各自优劣吧。\n","date":"2019-06-01T21:30:20+08:00","permalink":"https://blog.kagaya.fun/p/2019/pthread%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B%E5%88%9D%E6%8E%A2/","title":"pthread多线程编程初探"},{"content":"CSS伪类的应用——实现聊天气泡效果 在做聊天室的时候为了实现一个气泡消息的效果，于是目标是要实现一个类似b站私信消息的感觉\n主要原理是利用css伪类的:before和:after在圆角div前后插入两个方块，利用圆角叠加遮盖实现气泡的效果\n消息气泡的模板定义如下\n1 2 3 4 5 6 7 \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;message\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;bubble\u0026#34;\u0026gt; \u0026lt;slot\u0026gt;\u0026lt;/slot\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; 由于使用了vue，其中的\u0026lt;slot\u0026gt;为之后插入的消息内容\n首先将bubble设置为圆角矩形，因为要适应内部文本大小且要占据一行，因此定义为inline-block，并用div包裹\n1 2 3 4 5 6 7 8 9 .bubble{ display: inline-block; position: relative; padding: 15px; left: 0px; line-height: 1.2; background: #fff; border-radius: 15px; } 之后插入先后插入一个:before和:after块，位置定义为absolute（注意.bubble需定义为relative才能做到相对.bubble块偏移）\n:before\n1 2 3 4 5 6 7 8 9 10 .bubble:before { content: \u0026#39;\u0026#39;; position: absolute; top: -8px; left: -15px; height: 30px; width: 15px; background-color: #fff; border-bottom-left-radius: 40px 60px; } :after\n1 2 3 4 5 6 7 8 9 10 .bubble:after { content: \u0026#39;\u0026#39;; position: absolute; top: -8px; left: -15px; width: 15px; height: 20px; background-color: #f2f2f2; border-bottom-left-radius: 100px 40px; } 这里的background-color和背景色设置为相同\nborder-bottom-left-radius熟悉后的两个值分别表示椭圆的半长轴和半短轴，用来调整角的弯曲度\n值得注意的是这里实现的角右侧只能紧贴垂直的边，理论上通过调整top、left、width、height四个值应该可以实现将角移到圆角的部位\n最终效果虽然还是有点差别但还是很接近了，\n但是b站对于图片气泡直接使用了canvas实现，画一个气泡然后用图片填充\n(⊙﹏⊙)学到了\n","date":"2019-05-17T21:30:20+08:00","permalink":"https://blog.kagaya.fun/p/2019/css%E4%BC%AA%E7%B1%BB%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E7%8E%B0%E8%81%8A%E5%A4%A9%E6%B0%94%E6%B3%A1%E6%95%88%E6%9E%9C/","title":"CSS伪类的应用——实现聊天气泡效果"},{"content":"记人生中第一次面试 在准备暑期实习的时候第一个就投了微软，现在坐在面试后返回学校的地铁上，打开了md编辑器，准备写点什么。我想，无论结果如何，这都是对我来说十分有意义的一次体验。\n微软分网测和面试两个部分，投简历后不久就先收到了网测通知的邮件，也就是某天下午7点到9点要在电脑前完成一个测试。邮件里附了一个考试连接和一个体验连接，后者可以让你提前熟悉一下操作环境，测试一下你的电脑环境（摄像头）。对了，网测通知的全程要开摄像头，也就是说会有监考，内容是两小时做四道题。但是我测的时候全程摄像头的指示灯都没有亮，甚至倒计时的计时器都是6小时？？？\n考试界面和leetcode很像，题目都是英文的，前两道题比较简单，后两道难度一下就提高了。除非打过acm的选手或特别练过的，我觉得就算有思路也很难下手。当然我也只是完成了前两道，到了两小时就交卷了。\n没想到过了几天竟然受到了微软的面试通知，这确实让我蛮惊喜意外的，但更多的还是不安。因为对于面试经验几乎为0的我来说，第一次就是面微软确实是一个巨大的挑战。准备时间大概只有两周，于是翻开了各大论坛的面经指南寻找准备方法。一直在忙于做各种课设的我也没怎么刷leetcode，甚至面试前在车上还在看全排列的题（我甚至没想到看的这道题三小时后就考了！！！🙄🙃👻）\n面试是下午两点开始，由于我在嘉大荒，微软在闵大荒。3小时的路程让我不得不一大早9点就出发，结果还在微软的接待室坐了一小时。内心是紧张激动交织在一起，接待室微微的香味甚至让我一度想打瞌睡。后面又来了一波人好像是互相认识的，最后我们这波大概7、8个人的样子。HR小姐姐做完签到确认后就带我们到另外一个办公楼的休息室做等待。\n然后就是等面试官一个个敲门叫自己的名字，然后带你去另一个房间面试。 我遇到的第一个面试官感觉人很好，在穿过办公区的时候还给我做介绍（我只能感叹不愧是微软）。到房间坐下后，一开始先让我做自我介绍，然后问了几个我简历上写的做过的项目，接着就是白板做题。\n问题是机票按照先后顺序（出发地，目的地）排序的问题。我一开始先说了个谁都能想到的On2的方法，当然我肯定不会只到这。接着分析了下优化主要可以从查找效率入手，先是提了下二分搜索，最后还是落脚在了哈希表上。这样搜索基本可以达到常数效率。后来面试官和我一边讨论一些特殊情况，一边改进程序，感觉就像是老师学生在讨论题目，越说越有信心。感觉最后面试官也很满意，在带我回休息室的时候还问我要不要喝杯咖啡。\n第二个面试官就感觉不是很顺利了，叫我的时候问我有没有简历（我：？？？），理论上每个面试官应该收到的都是电子版的简历，有些比较负责的可能会先去看你的简历，熟悉了以后在针对性的面试，所以他这么一问我只能说没有。于是他只能拿出手机看电子版简历，不过明显是第一次看，第一个就问我研究方向是什么？（我：？？？），我说明了下我是本科生后他好像有点失望，便将提问方向转向了有没有写代码的机会。我只能说代码肯定每天都在写，只是没有接触大项目的机会。于是同样问了几个之前的项目就开始做题，题目正是我两小时前还在看的全排列。我还有点惊喜，因为思路立马就有了，于是直接开始写。没用多久就完成了，是一个很简单的递归程序，我用最保险的方法写了。写完后便开始挑毛病，我发现我漏了一句pop，不过其他基本没什么逻辑上的错误。最后他觉得我写的复杂了，自己说了个插入的方法，确实这个方法我也看到过，但是不是很熟就没写。简单提了下他自己的思路后就结束了，二面大概只面了半小时，而且我觉得沟通不是很舒服，我感觉多半是凉了。于是两轮后就没有三轮了。\n不过今天一天确实收获不少，很荣幸能微软一日游，也为今后如何准备面试提供了不少经验，感觉还是赚到了，给自己加油！\n","date":"2019-05-07T21:30:20+08:00","permalink":"https://blog.kagaya.fun/p/2019/%E8%AE%B0%E4%BA%BA%E7%94%9F%E4%B8%AD%E7%AC%AC%E4%B8%80%E6%AC%A1%E9%9D%A2%E8%AF%95/","title":"记人生中第一次面试"},{"content":"SQL变量总结 变量分类 用户变量 以@开始，形式为@变量名 全局变量 set global 变量名 或者 set @@global.变量名 对所有客户端生效 只有具有super权限才可以设置全局变量 会话变量 用户变量与mysql客户端绑定，只对当前用户使用的客户端生效 只对连接的客户端生效 局部变量 declare专门用于声明 作用范围在begin和end语句块之间 定义方法 \u0026ldquo;=\u0026quot;,如 set @a =3,@a:=5 \u0026ldquo;:=\u0026quot;。select常常这样使用 set可以使用以上两种形式设置变量。而select只能使用\u0026rdquo;:=\u0026ldquo;的形式设置变量 未定义的变量初始化是null 示例题目： X 市建了一个新的体育馆，每日人流量信息被记录在这三列信息中：序号 (id)、日期 (date)、 人流量 (people)。\n请编写一个查询语句，找出高峰期时段，要求连续三天及以上，并且每天人流量均不少于100。\nid visit_date people 1 2017-01-01 10 2 2017-01-02 109 3 2017-01-03 150 4 2017-01-04 99 5 2017-01-05 145 6 2017-01-06 1455 7 2017-01-07 199 8 2017-01-08 188 对于上面实例数据，应当输出\nid visit_date people 5 2017-01-05 145 6 2017-01-06 1455 7 2017-01-07 199 8 2017-01-08 188 利用会话变量+巧用排序实现，利用两个中间表逐渐接近目标（个人偏向使用这种）\n1 2 3 4 5 6 7 8 9 10 11 select id, visit_date, people from ( select id, visit_date, people, if((days \u0026gt; 0 and @ok = 1) or days \u0026gt;= 3, @ok:=1, @ok:=0) as ok from ( select id, visit_date, people, if(people \u0026gt;= 100, @days:=@days+1, @days:=0) as days from stadium, (select @days := 0) as t1 ) countedDays, (select @ok := 0) as t2 order by id desc ) oklist where ok = 1 order by id asc; 再提供一种思路：\n利用多表联合查询，分为三种情况顺序，思路十分巧妙，代码短小精悍，distinct巧妙的防止了重复选择\n1 2 3 4 5 6 7 select distinct a.* from stadium a,stadium b,stadium c where a.people\u0026gt;=100 and b.people\u0026gt;=100 and c.people\u0026gt;=100 and ( (a.id = b.id-1 and b.id = c.id -1) or (a.id = b.id-1 and a.id = c.id +1) or (a.id = b.id+1 and b.id = c.id +1) ) order by a.id by Derrick\n","date":"2019-04-07T21:30:20+08:00","permalink":"https://blog.kagaya.fun/p/2019/sql%E5%8F%98%E9%87%8F%E6%80%BB%E7%BB%93/","title":"SQL变量总结"},{"content":"Mysql和MariaDB中的Rank排序的坑 题目中需要对一个成绩表按考试科目以及学生做排名\n为了加快之后其他步骤的查询速度，我想着将所有成绩都添加好排名后建立一个临时表\n想法是\n先将score表中按考试课程升序，考试成绩降序，学号后两位升序做子查询 在新表中调用自定义的rank函数添加排名 rank函数是这样写的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /* 临时rank function 要求原表按id score_mark降序 排列好 */ delimiter // drop function if exists myrank; create function myrank(score_id int, score_mark float) returns int READS SQL DATA begin if(@pre_score_id is null or @pre_score_id != score_id) then set @pre_score_id := score_id; set @pre_score_mark := score_mark; set @row := 1; set @rank := 1; else set @row := @row + 1; if(@pre_score_mark != score_mark or score_mark is null) then set @rank := @row; set @pre_score_mark = score_mark; end if; end if; return @rank; end // delimiter ; 完整的临时表建立过程如下，看起来结构很简单，只是对已经排好序的表统计排名：\n1 2 3 4 5 6 7 8 drop table if exists tmp_table_rank; create table tmp_table_rank as select score_id, score_stuno, Score_mark, myrank(score_id, Score_mark) as rank from ( select * from score order by score_id asc, Score_mark desc, right(score_stuno, 2) asc ); 写完后在个人的云服务器上测试了一下结果符合预期，但为了保险起见，又在虚拟机上测试了同样数据和同样的代码，结果却不一样！！！\n？？？\n首先怀疑的是mysql版本的问题\n服务器上使用的mysql版本为mysql Ver 14.14 Distrib 5.6.37, for linux-glibc2.12 (x86_64) using EditLine wrapper\n虚拟机上则是mysql Ver 15.1 5.5.56-mariaDB\n按理说mariaDB本质上还是mysql，结果却不一样。。。\n在询问同学和老师后怀疑是两者在排序和调用函数的处理上有不同，理论上期望mysql先做完子查询的排序后，对排好序的表计算rank，mysql确实是这么做的，但是mariaDB似乎还没做完子查询的排序就开始调用函数计算rank，导致每个人的成绩和排名对应不上，出现错误\n针对这个怀疑，做出了如下修改：\n先将子查询放入一个临时表中，再调用该临时表\n1 2 3 4 5 6 7 8 9 10 11 /* 临时table, 按科目添加排名 */ drop table if exists tmp_table_asc; create table tmp_table_asc as select * from score order by score_id asc, Score_mark desc, right(score_stuno, 2) asc; drop table if exists tmp_table_rank; create table tmp_table_rank as select score_id, score_stuno, Score_mark, myrank(score_id, Score_mark) as rank from tmp_table_asc; 结果正确了，果然你俩还不是一个东西啊（苦笑\n","date":"2019-04-01T21:30:20+08:00","permalink":"https://blog.kagaya.fun/p/2019/mysql%E5%92%8Cmariadb%E4%B8%AD%E7%9A%84rank%E6%8E%92%E5%BA%8F%E7%9A%84%E5%9D%91/","title":"Mysql和MariaDB中的Rank排序的坑"},{"content":"难得看次电影，随便写写 ​\t趁着周五+惊奇队长第一天上映，拉着基友去学校边上的电影院一睹为快，顺便看了广受好评的奥斯卡最佳影片——Green Book 绿皮书\n​\t不得不说我真是很庆幸自己先看了惊奇队长，再看了绿皮书，然后心满意足地回到寝室。先说说惊奇队长，作为复仇者联盟的最强战力，上映的这部《惊奇队长》基本上是为了一个月后的《复联4》做铺垫的。一方面为了新加入的成员交代好故事背景，另一方面补充一些主线上的剧情。总得来说没有什么惊喜的地方，哦，对我来说最惊喜的地方是能再次在荧幕上看到富瑞和科尔森，两位在神盾局中举足轻重的人物。说真的，富瑞那时候还没有戴他那个标志性的眼罩，而且显得非常年轻，体态虽然宽大但丝毫不显臃肿，甚至当时作为三级特工的富瑞已经显得十分老练。刚出场的时候我一时还没认出来，直到科尔森喊出富瑞的名字，我惊喜的差点叫出来。富瑞的形象一直是那样稳重，看到他总会给人一种莫名的安全感，仿佛一切都在他的掌握之中。惊喜之余，也顺便给神盾局上柱香。另外，关于剧情主要还是意料之中地交代了惊奇队长身世以及能力的来源，虽然可能没有人看这篇blog，但我还是没有剧透的习惯，就不细说了，个人觉得就算没有看《惊奇队长》对于接下来的复联也不会有什么影响，比较关键的内容就是宇宙魔方是怎么到富瑞手里的以及富瑞在《复联3》里联系惊奇队长的那个寻呼机的来历，不过全身发出耀眼光芒的女侠还是蛮帅的，就凭一个人就能单挑一支舰队的能力（特效）来看，还是非常适合边吃爆米花边看的。\n​\t看完《惊奇队长》紧接着就去看了一直被各种吹爆的《绿皮书》，没错，我也要吹爆“小绿本”了。之前看到海报上一个人开着一辆绿色的车，我大致就可以猜到这可能是一部公路电影，但不清楚具体的内容，如果没有获得这么高的舆论评价我可能也不会在影院下架前赶着去看了最后一场。对这类题材其实并不是很感冒，而且看过的公路片印象深的也并不多（如果阿甘也算的话），看完以后确实感叹，小绿本是真的物有所值，它值得那个小金人。\n​\t《绿皮书》的核心内容其实还是黑人与种族歧视问题，对于这种在美国算作政治正确题材的电影，就像国内抗日爱国题材的电影，总能成为稳定的创作素材，的确能让我获得如此舒服的观后体验着实出乎我的意料。尤其是整部电影从头到尾基本都保持在一个轻松愉快的气氛中，甚至在刚开始我误以为它是一部喜剧片。影片一开始展示的“本片改编自真实故事”确实就已经吸引了我的注意力，这种真人真事的电影确实更能说服观众。大致的内容就是一名退役的白人士兵，托尼，为了生计给一位黑人钢琴家，雪利，做司机，一路上陪伴他在美国的南部进行了两个月的巡演的故事。时间背景大致是在上个世纪，种族歧视在美国南部盛行的年代。这样题材的电影也很容易就落入俗套，比如很可能就是黑人兄弟们是怎样活在白人歧视和压迫下一步一步克服艰难获得成功的故事。当然，这样也可以是一部好电影，比如《42号传奇》，我还蛮喜欢这部的，推荐。\n​\t《绿皮书》中的黑人主角一开始就是功成名就的黑人音乐家，甚至受到总统邀请在白宫演奏过两次。于是，这次站在这样新颖一个角度来看待这样一个严肃的问题，编剧也似乎十分强调雪利的身份来营造一种对立，制造矛盾体，要知道，对于黑人中混的比较好的人，面对种族歧视时往往是白人黑人两边都不讨好，片中也有不少用雪利和在南方生活的普通黑人对比的镜头，有些还是蛮令人心酸的，比如在公路上修车那段，路边农场里的黑人农民隔着栅栏望着车里后座上的雪利，看着一位白人在给他开门，修车，大家不约而同地停下手中地农活，就这样一言不发地默默注视着雪利。这个片段从头到尾没有一句台词，长度可能也就几十秒，但是确是我最喜欢，意义最丰富的一段。\n​\t好的电影可能没必要通过悲伤的情景把你感动到流泪，或通过激烈的场面和配乐让你热血沸腾。也许可以是这样一部轻松愉快，让观影者不时发出欢笑声的电影，也能引发人们对问题矛盾的无限深思。\n2019年3月8日23点13分\n","date":"2019-03-08T21:30:20+08:00","permalink":"https://blog.kagaya.fun/p/2019/%E9%9A%BE%E5%BE%97%E7%9C%8B%E6%AC%A1%E7%94%B5%E5%BD%B1%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/","title":"难得看次电影，随便写写"},{"content":"DitF第十三集ed绘本《魔物与王子》翻译 一个挺悲伤的故事（有翻译错误还请指教）\n在某个国家漆黑的森林深处，有一个不为人知的种族生活在那里。背上长着巨大的他们十分的美丽，但是确实被成为“魔物”般的存在。那里生活着的“魔物”的公主。她的背上也有着巨大的灰色翅膀，当她成长到十六岁的时候，就被允许用翅膀飞到森林的外面。\n然后，当过完十六岁生日的时候，公主第一次飞出森林，穿越过了险峻的山峰，飞过了河流。好不容易到达了一个住着不同种族的地方——人类的国家。空中的月光照映着她的脸庞。\n她降落在了大城市中的庭院里，看到了一个独自仰望着月亮的青年的身影。公主躲在树林中注视着青年的身影。那是第一次萌生了恋爱的感觉。但是，他明显和自己是不同的种族。从来没有魔物和人类结成伴侣的事情发生。\n公主便去拜访了住在森林里的魔女，对魔女说：“我想和人类一起生活，想和那个人结婚”“好啊，用你的翅膀来交换，但是你要记住。无论伪装成什么样，只要身为魔物的你在他身边，就会吞噬王子的生命。” 当背上巨大的翅膀被取下，至今从未感受过的恐惧向公主袭来，再也无法飞上天空。即使这样公主还是很高兴，边哭边笑的说到：“人类！人类！我变得和那个人一样了”\n于是，公主再次用自己的脚向人类的国家走去，在沙漠中遇到了人影。“不好了！谁能来救救这个人啊！”那里有一个脚被毒蛇咬了的青年的身影。公主急忙上去亲自把毒从被蛇咬的伤口中吸出来。“谢谢你。多么勇敢的人啊。我是这个国家的王子” 这位王子就是公主在那天城里遇到的青年。“你是救了我命的人，请和我结婚。”\n很快举行了结婚仪式，神父询问穿着洁白婚纱的公主，“你是否发誓，无论疾病还是健康的时候，至死都不会把你们分开” “是的，我发誓“ 互相交换了结婚戒指，在神父的指引下立下誓约之吻，”让我们祝福救了王子性命的这位勇敢的少女！“ 在巨大的欢呼声中，整个国家都洋溢着喜悦的气氛。\n“人类，人类！即使没有能飞上天空的强力的翅膀，也没有尖锐的牙齿和爪子。弱小无力的生物，确是这样的温柔。人类真是太棒了” 之后，作为人类国家的公主，度过了一个又一个幸福的日子。在旁边一直支持着王子的工作。时常好好地游览了各种各样的地方，如蓝宝石般散发着光芒的大海、闪闪发光的草原、像燃烧的火焰般的极光。公主握着王子的手，微笑着看着他“我们不会分开的对吧。xxxx（模糊）”\n但是，公主的幸福并没有持续很久，在身体如同灼烧一般的疼痛中醒来，自己的身体也渐渐变成一个丑陋的怪物，尖锐的利爪迅速的生长。“为什么？我应该已经拜托了她让我成为了人类啊”，然后，她的背后丑陋漆黑的翅膀突破皮肉展开来。“既然使用了魔法也是有代价的。你已经作为完美的人类获得了幸福了吧，之后你自己只是会丧失自己，变成丑陋得怪物而已罢了。” 森之魔女的声音一句句传到她的耳边。“但是，只要你亲手杀掉你最亲爱的人，你就会解除诅咒，变回你最初始的样子”\n公主看着自己心爱的王子，将自己长长的爪子伸向了王子的脖子。明明是比谁都要喜爱的人啊，如今却涌来了想要切断他的喉咙杀死他的感觉。公主拼命的控制自己，自己的手也渐渐渗出了血。“只要杀掉王子，诅咒就会解除……” 公主流着泪，轻轻的亲了下王子的脸颊。\n王子醒来后发现公主并不在身边，而是在床上发现了许多漆黑的羽毛。王子十分悲伤，即使在全城中寻找，却再也没有人见过公主。\n","date":"2019-01-21T21:30:20+08:00","image":"https://blog.kagaya.fun/p/2019/ditf%E7%AC%AC%E5%8D%81%E4%B8%89%E9%9B%86ed%E7%BB%98%E6%9C%AC%E9%AD%94%E7%89%A9%E4%B8%8E%E7%8E%8B%E5%AD%90%E7%BF%BB%E8%AF%91/67015109_p0_hu_755f6e753f2cd2e0.jpg","permalink":"https://blog.kagaya.fun/p/2019/ditf%E7%AC%AC%E5%8D%81%E4%B8%89%E9%9B%86ed%E7%BB%98%E6%9C%AC%E9%AD%94%E7%89%A9%E4%B8%8E%E7%8E%8B%E5%AD%90%E7%BF%BB%E8%AF%91/","title":"DitF第十三集ed绘本《魔物与王子》翻译"},{"content":"我们都在向前走，只有回忆驻留在原地 电影《肆式青春》短评\n观看建议：这部电影是三位导演通过中国这个舞台来描绘他们眼中青春的故事，三个篇章之间关系不大，但各自讲述一段完整的故事，虽然篇幅较短，但请用心体会，如果能在将自己的经历带入其中一段故事中的话能获得最好的观看体验。\n首先很遗憾，限于排片过少没能够第一时间去影院支持一下。其实很早就开始关注这部电影了，随着中国资本进入动画市场，也一直期待着有能有一部讲述中国自己故事的动画，不得不说，绘梦这次做的不错。整体看完以后觉得基本上没让我失望，个人看来第一篇与第三篇是比较出彩的，可以看到叫兽和李导真的是十分想把属于那个时代的回忆和情感通过动画的形式传达出来，各个细节十分到位，电影中的各种中国式场景想必各位也看得十分有亲切感。也许限于篇幅原因，有许多内容没能够呈现出来，但也确实足够表现出两位在中国这片土地上长大的年轻人对于那个一段美好时光的不舍之情，叙事节奏的把握我觉得真的是十分不错了。但我觉得缺点也是有的，题材的限制使得只有相似经历的人更容易在这个电影里获得共鸣，另外篇幅限制使得许多地方存在铺垫不足的问题。但个人认为这些不至于影响到整个电影所表达的东西，所以我还是会推荐更多人来看的。 正如《一碗乡愁》的导演叫兽所说的一样，他的目的是要引起一部分人的共鸣，如果你在看这部电影的时候，电影中的一个事物，一个情节，或者一个人物，能让你不由自主地将自己代入到电影之中，那可以说这部电影是成功的。无论是自己的那位和蔼的长辈，还是小时候最喜欢的一道美食，抑或是那个曾经羞涩的自己所暗恋的ta。尽管每个人的青春时光都不尽相同，但大家都经历过那个年代，总能找到一些只属于那个年代的印记，电影讲述的不只是导演们自己的故事，也是在讲述我们每一个80后，90后甚至00后的青春。\n我们都在向前走，只有回忆驻留在原地。如今，我也只能借助这样一部电影，才能够回忆起曾经那个傻傻的自己了。\n","date":"2019-01-18T21:30:20+08:00","image":"https://blog.kagaya.fun/p/2019/flavors-of-youth/ssqc_hu_2b37a31b8ed38e2c.jpeg","permalink":"https://blog.kagaya.fun/p/2019/flavors-of-youth/","title":"我们都在向前走，只有回忆驻留在原地"},{"content":"打算重启kgyzone 离建好kgyzone也过去一年多了，本来想好好在这里总结一下身边发生的事情，或是做一些技术记录，然而一直没有掌握worldpress的正确使用方法，人嘛，总想在走过的地方留下点什么东西，哪怕是一个脚印也好，好证明自己也曾存在过。\n之后可以慢慢在这里发一下心得体会。或许可以直接将markdown格式的文件直接发上来，那样的话会方便很多。\nmarkdown真好用啊\n","date":"2019-01-01T21:30:20+08:00","permalink":"https://blog.kagaya.fun/p/2019/%E6%89%93%E7%AE%97%E9%87%8D%E5%90%AFkgyzone/","title":"打算重启kgyzone"}]